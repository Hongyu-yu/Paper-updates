# The Operando Nature of Isobutene Adsorbed in Zeolite H−SSZ−13 Unraveled by Machine Learning Potentials Beyond DFT Accuracy

## 通过超出DFT精度的机器学习潜力揭示了h-ssz − 13沸石中吸附的异丁烯的操作性质

Link: https://onlinelibrary.wiley.com/doi/10.1002/anie.202413637?af=R

Angewandte Chemie International Edition, EarlyView.


---
# [ASAP] Augmenting Human Expertise in Weighted Ensemble Simulations through Deep Learning-Based Information Bottleneck

## [ASAP] 通过基于深度学习的信息瓶颈增强加权集成模拟中的人类专业知识

Link: http://dx.doi.org/10.1021/acs.jctc.4c00919

<p><img alt="TOC Graphic" src="https://pubs.acs.org/cms/10.1021/acs.jctc.4c00919/asset/images/medium/ct4c00919_0014.gif" /></p><div><cite>Journal of Chemical Theory and Computation</cite></div><div>DOI: 10.1021/acs.jctc.4c00919</div>


---
# Noise classification in three-level quantum networks by Machine Learning

## 基于机器学习的三层量子网络噪声分类

Link: http://iopscience.iop.org/article/10.1088/2632-2153/ad9193

We investigate a machine learning based classification of noise acting on a small quantum network with the aim of detecting spatial or multilevel correlations, and the interplay with Markovianity. We control a three-level system by inducing coherent population transfer exploiting different pulse amplitude combinations as inputs to train a feedforward neural network. We show that supervised learning can classify different types of classical dephasing noise affecting the system. Three non-Markovian (quasi-static correlated, anti-correlated and uncorrelated) and Markovian noises are classified with more than 99% accuracy. On the contrary, correlations of Markovian noise cannot be discriminated with our method. Our approach is robust to statistical measurement errors and retains its effectiveness for physical measurements where only a limited number of samples is available making it very experimental-friendly. Our result paves the way for classifying spatial correlations of noise in quantum architectures.


---
# Impact of data bias on machine learning for crystal compound synthesizability predictions

## 数据偏差对晶体化合物可合成性预测的机器学习的影响

Link: http://iopscience.iop.org/article/10.1088/2632-2153/ad9378

Machine learning models are susceptible to being misled by biases in training data that emphasize incidental correlations over the intended learning task. In this study, we demonstrate the impact of data bias on the performance of a machine learning model designed to predict the likelihood of synthesizability of crystal compounds. The model performs a binary classification on labeled crystal samples. Despite using the same architecture for the machine learning model, we showcase how the model’s learning and prediction behavior differs once trained on distinct data. We use two data sets for illustration: a mixed-source data set that integrates experimental and computational crystal samples and a single-source data set consisting of data exclusively from one computational database. We present simple procedures to detect data bias and to evaluate its effect on the model’s performance and generalization. This study reveals how inconsistent, unbalanced data can propagate bias, undermining real-world applicability even for advanced machine learning techniques.


---
# Data-driven Modeling of Granular Chains with Modern Koopman Theory

## 基于现代Koopman理论的颗粒链数据驱动建模

Link: https://arxiv.org/abs/2411.15142

arXiv:2411.15142v1 Announce Type: new 
Abstract: Externally driven dense packings of particles can exhibit nonlinear wave phenomena that are not described by effective medium theory or linearized approximate models. Such nontrivial wave responses can be exploited to design sound-focusing/scrambling devices, acoustic filters, and analog computational units. At high amplitude vibrations or low confinement pressures, the effect of nonlinear particle contacts becomes increasingly noticeable, and the interplay of nonlinearity, disorder, and discreteness in the system gives rise to remarkable properties, particularly useful in designing structures with exotic properties. In this paper, we build upon the data-driven methods in dynamical system analysis and show that the Koopman spectral theory can be applied to granular crystals, enabling their phase space analysis beyond the linearizable regime and without recourse to any approximations considered in the previous works. We show that a deep neural network can map the dynamics to a latent space where the essential nonlinearity of the granular system unfolds into a high-dimensional linear space. As a proof of concept, we use data from numerical simulations of a two-particle system and evaluate the accuracy of the trajectory predictions under various initial conditions. By incorporating data from experimental measurements, our proposed framework can directly capture the underlying dynamics without imposing any assumptions about the physics model. Spectral analysis of the trained surrogate system can help bridge the gap between the simulation results and the physical realization of granular crystals and facilitate the inverse design of materials with desired behaviors.


---
# Accelerating CALPHAD-based Phase Diagram Predictions in Complex Alloys Using Universal Machine Learning Potentials: Opportunities and Challenges

## 使用通用机器学习潜力加速复杂合金中基于calphed的相图预测: 机遇与挑战

Link: https://arxiv.org/abs/2411.15351

arXiv:2411.15351v1 Announce Type: new 
Abstract: Accurate phase diagram prediction is crucial for understanding alloy thermodynamics and advancing materials design. While traditional CALPHAD methods are robust, they are resource-intensive and limited by experimentally assessed data. This work explores the use of machine learning interatomic potentials (MLIPs) such as M3GNet, CHGNet, MACE, SevenNet, and ORB to significantly accelerate phase diagram calculations by using the Alloy Theoretic Automated Toolkit (ATAT) to map calculations of the energies and free energies of atomistic systems to CALPHAD-compatible thermodynamic descriptions. Using case studies including Cr-Mo, Cu-Au, and Pt-W, we demonstrate that MLIPs, particularly ORB, achieve computational speedups exceeding three orders of magnitude compared to DFT while maintaining phase stability predictions within acceptable accuracy. Extending this approach to liquid phases and ternary systems like Cr-Mo-V highlights its versatility for high-entropy alloys and complex chemical spaces. This work demonstrates that MLIPs, integrated with tools like ATAT within a CALPHAD framework, provide an efficient and accurate framework for high-throughput thermodynamic modeling, enabling rapid exploration of novel alloy systems. While many challenges remain to be addressed, the accuracy of some of these MLIPs (ORB in particular) are on the verge of paving the way toward high-throughput generation of CALPHAD thermodynamic descriptions of multi-component, multi-phase alloy systems.


---
# Graph Transformer Networks for Accurate Band Structure Prediction: An End-to-End Approach

## 用于精确频带结构预测的图变压器网络: 端到端方法

Link: https://arxiv.org/abs/2411.16483

arXiv:2411.16483v1 Announce Type: new 
Abstract: Predicting electronic band structures from crystal structures is crucial for understanding structure-property correlations in materials science. First-principles approaches are accurate but computationally intensive. Recent years, machine learning (ML) has been extensively applied to this field, while existing ML models predominantly focus on band gap predictions or indirect band structure estimation via solving predicted Hamiltonians. An end-to-end model to predict band structure accurately and efficiently is still lacking. Here, we introduce a graph Transformer-based end-to-end approach that directly predicts band structures from crystal structures with high accuracy. Our method leverages the continuity of the k-path and treat continuous bands as a sequence. We demonstrate that our model not only provides accurate band structure predictions but also can derive other properties (such as band gap, band center, and band dispersion) with high accuracy. We verify the model performance on large and diverse datasets.


---
# Reflections from the 2024 Large Language Model (LLM) Hackathon for Applications in Materials Science and Chemistry

## 2024大型语言模型 (LLM) Hackathon在材料科学和化学中的应用

Link: https://arxiv.org/abs/2411.15221

arXiv:2411.15221v1 Announce Type: cross 
Abstract: Here, we present the outcomes from the second Large Language Model (LLM) Hackathon for Applications in Materials Science and Chemistry, which engaged participants across global hybrid locations, resulting in 34 team submissions. The submissions spanned seven key application areas and demonstrated the diverse utility of LLMs for applications in (1) molecular and material property prediction; (2) molecular and material design; (3) automation and novel interfaces; (4) scientific communication and education; (5) research data management and automation; (6) hypothesis generation and evaluation; and (7) knowledge extraction and reasoning from scientific literature. Each team submission is presented in a summary table with links to the code and as brief papers in the appendix. Beyond team results, we discuss the hackathon event and its hybrid format, which included physical hubs in Toronto, Montreal, San Francisco, Berlin, Lausanne, and Tokyo, alongside a global online hub to enable local and virtual collaboration. Overall, the event highlighted significant improvements in LLM capabilities since the previous year's hackathon, suggesting continued expansion of LLMs for applications in materials science and chemistry research. These outcomes demonstrate the dual utility of LLMs as both multipurpose models for diverse machine learning tasks and platforms for rapid prototyping custom applications in scientific research.


---
# Proportional infinite-width infinite-depth limit for deep linear neural networks

## 深度线性神经网络的比例无限宽无限深极限

Link: https://arxiv.org/abs/2411.15267

arXiv:2411.15267v1 Announce Type: cross 
Abstract: We study the distributional properties of linear neural networks with random parameters in the context of large networks, where the number of layers diverges in proportion to the number of neurons per layer. Prior works have shown that in the infinite-width regime, where the number of neurons per layer grows to infinity while the depth remains fixed, neural networks converge to a Gaussian process, known as the Neural Network Gaussian Process. However, this Gaussian limit sacrifices descriptive power, as it lacks the ability to learn dependent features and produce output correlations that reflect observed labels. Motivated by these limitations, we explore the joint proportional limit in which both depth and width diverge but maintain a constant ratio, yielding a non-Gaussian distribution that retains correlations between outputs. Our contribution extends previous works by rigorously characterizing, for linear activation functions, the limiting distribution as a nontrivial mixture of Gaussians.


---
# Understanding Machine Learning Paradigms through the Lens of Statistical Thermodynamics: A tutorial

## 通过统计热力学的镜头理解机器学习范例: 教程

Link: https://arxiv.org/abs/2411.15945

arXiv:2411.15945v1 Announce Type: cross 
Abstract: This tutorial investigates the convergence of statistical mechanics and learning theory, elucidating the potential enhancements in machine learning methodologies through the integration of foundational principles from physics. The tutorial delves into advanced techniques like entropy, free energy, and variational inference which are utilized in machine learning, illustrating their significant contributions to model efficiency and robustness. By bridging these scientific disciplines, we aspire to inspire newer methodologies in researches, demonstrating how an in-depth comprehension of physical systems' behavior can yield more effective and dependable machine learning models, particularly in contexts characterized by uncertainty.


---
# Machine learning for cerebral blood vessels' malformations

## 机器学习在脑血管畸形中的应用

Link: https://arxiv.org/abs/2411.16349

arXiv:2411.16349v1 Announce Type: cross 
Abstract: Cerebral aneurysms and arteriovenous malformations are life-threatening hemodynamic pathologies of the brain. While surgical intervention is often essential to prevent fatal outcomes, it carries significant risks both during the procedure and in the postoperative period, making the management of these conditions highly challenging. Parameters of cerebral blood flow, routinely monitored during medical interventions, could potentially be utilized in machine learning-assisted protocols for risk assessment and therapeutic prognosis. To this end, we developed a linear oscillatory model of blood velocity and pressure for clinical data acquired from neurosurgical operations. Using the method of Sparse Identification of Nonlinear Dynamics (SINDy), the parameters of our model can be reconstructed online within milliseconds from a short time series of the hemodynamic variables. The identified parameter values enable automated classification of the blood-flow pathologies by means of logistic regression, achieving an accuracy of 73 %. Our results demonstrate the potential of this model for both diagnostic and prognostic applications, providing a robust and interpretable framework for assessing cerebral blood vessel conditions.


---
# Global and local synchrony of coupled neurons in small-world networks

## 小世界网络中耦合神经元的全局和局部同步

Link: https://arxiv.org/abs/2411.16374

arXiv:2411.16374v1 Announce Type: cross 
Abstract: Synchronous firing of neurons is thought to play important functional roles such as feature binding and switching of cognitive states. Although synchronization has mainly been investigated using model neurons with simple connection topology so far, real neural networks have more complex structures. Here we examine behavior of pulse-coupled leaky integrate-and-fire neurons with various network structures. We first show that the dispersion of the number of connections for neurons influences dynamical behavior even if other major topological statistics are kept fixed. The rewiring probability parameter representing the randomness of networks bridges two spatially opposite frameworks: precise local synchrony and rough global synchrony. Finally, cooperation of the global connections and the local clustering property, which is prominent in small-world networks, reinforces synchrony of distant neuronal groups receiving coherent inputs.


---
# Why hyperdensity functionals describe any equilibrium observable

## 为什么超密度泛函描述任何可观察的平衡

Link: https://arxiv.org/abs/2410.10534

arXiv:2410.10534v2 Announce Type: replace 
Abstract: We give an introductory account of the recent hyperdensity functional theory for the equilibrium statistical mechanics of soft matter systems [F. Samm\"uller et al., Phys. Rev. Lett. 133, 098201 (2024); 10.1103/PhysRevLett.133.098201]. Hyperdensity functionals give access to the behaviour of arbitrary thermal observables in spatially inhomogeneous equilibrium many-body systems. The approach is based on classical density functional theory applied to an extended ensemble using standard functional techniques. The associated formally exact generalized Mermin-Evans functional relationships can be represented accurately by neural functionals. These neural networks are trained via simulation-based supervised machine learning and they allow one to carry out efficient functional calculus using automatic differentiation and numerical functional line integration. Exact sum rules, including hard wall contact theorems and hyperfluctuation Ornstein-Zernike equations, interrelate the different correlation functions. We lay out close connections to hyperforce correlation sum rules [S. Robitschko et al., Commun. Phys. 7, 103 (2024); 10.1038/s42005-024-01568-y] that arise from statistical mechanical gauge invariance [J. M\"uller et al., Phys. Rev. Lett. 133, 217101 (2024); 10.1103/PhysRevLett.133.217101]. Further quantitative measures of collective self-organization are provided by hyperdirect correlation functionals and spatially resolved hyperfluctuation profiles. The theory facilitates to gain deep insight into the inherent structuring mechanisms that govern the behaviour of both simple and complex order parameters in coupled many-body systems.


---
# A Comparison of Machine Learning Algorithms for Predicting Sea Surface Temperature in the Great Barrier Reef Region

## 预测大堡礁地区海表温度的机器学习算法比较

Link: https://arxiv.org/abs/2411.15202

arXiv:2411.15202v1 Announce Type: new 
Abstract: Predicting Sea Surface Temperature (SST) in the Great Barrier Reef (GBR) region is crucial for the effective management of its fragile ecosystems. This study provides a rigorous comparative analysis of several machine learning techniques to identify the most effective method for SST prediction in this area. We evaluate the performance of ridge regression, Least Absolute Shrinkage and Selection Operator (LASSO), Random Forest, and Extreme Gradient Boosting (XGBoost) algorithms. Our results reveal that while LASSO and ridge regression perform well, Random Forest and XGBoost significantly outperform them in terms of predictive accuracy, as evidenced by lower Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Prediction Error (RMSPE). Additionally, XGBoost demonstrated superior performance in minimizing Kullback- Leibler Divergence (KLD), indicating a closer alignment of predicted probability distributions with actual observations. These findings highlight the efficacy of using ensemble methods, particularly XGBoost, for predicting sea surface temperatures, making them valuable tools for climatological and environmental modeling.


---
# Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered Images for Online Breath-hold Reproducibility Verification of Liver Stereotactic Body Radiation Therapy

## 基于深度学习的kV触发图像肝脏穹顶自动勾画，用于肝脏立体定向放射治疗的在线屏气可重复性验证

Link: https://arxiv.org/abs/2411.15322

arXiv:2411.15322v1 Announce Type: new 
Abstract: Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally invasive treatment method for liver cancer and liver metastases. However, the effectiveness of SBRT relies on the accurate delivery of the dose to the tumor while sparing healthy tissue. Challenges persist in ensuring breath-hold reproducibility, with current methods often requiring manual verification of liver dome positions from kV-triggered images. To address this, we propose a proof-of-principle study of a deep learning-based pipeline to automatically delineate the liver dome from kV-planar images. From 24 patients who received SBRT for liver cancer or metastasis inside liver, 711 KV-triggered images acquired for online breath-hold verification were included in the current study. We developed a pipeline comprising a trained U-Net for automatic liver dome region segmentation from the triggered images followed by extraction of the liver dome via thresholding, edge detection, and morphological operations. The performance and generalizability of the pipeline was evaluated using 2-fold cross validation. The training of the U-Net model for liver region segmentation took under 30 minutes and the automatic delineation of a liver dome for any triggered image took less than one second. The RMSE and rate of detection for Fold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2 with 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3% respectively.


---
# Path to Low-Cost Direct Air Capture

## 低成本直接空气捕获之路

Link: https://arxiv.org/abs/2411.15369

arXiv:2411.15369v1 Announce Type: new 
Abstract: It is now accepted that gigatonnes of Carbon Dioxide Removal (CDR) from the atmosphere are needed to avoid the threat of catastrophic climate change. Direct Air Capture (DAC) is a promising scalable CDR with a relatively small environmental footprint. But questions about DAC cost and energy use remain that are delaying the needed DAC policy decisions to create a mobilization effort like was done in the Manhattan Project and to address the Covid crisis. Global Thermostat (GT) has publicly claimed costs of under 50 dollars per tonne for mature GT technology deployed at a climate relevant scale. Why this low DAC cost is achievable will be addressed by a simplified analysis of generic DAC costs and using that analysis combined with experimental data to evaluate GT's DAC technology. A detailed cost analysis of different approaches to DAC by the National Academy of Sciences (NAS) found an approach to DAC that had a learning cost limit as low as 25 dollars per tonne GT's DAC technology will be shown in Appendix 1 to have the same performance characteristics of the lowest-cost DAC identified in the NAS study. Thus, like solar costs, DAC costs can be reduced by learning by doing, but in the case of DAC, only one order of magnitude in cost reduction is needed. Therefore DAC technology can reach its low learning by doing cost limit at a scale much smaller than necessary to address climate change. From a climate perspective, current DAC embodiments costs and scale have less relevance than their learning curve cost limit. While GT's technology has demonstrated the crucial performance parameters to achieve a low-cost DAC, no inference should be drawn that other approaches cannot achieve low or lower cost, if they can demonstrate the crucial performance parameters. Continued R&amp;D on those performance parameters is needed.


---
# Minimizing Nature's Cost: Exploring Data-Free Physics-Informed Neural Network Solvers for Fluid Mechanics Applications

## 最小化自然的成本: 探索无数据的物理通知神经网络求解器的流体力学应用

Link: https://arxiv.org/abs/2411.15410

arXiv:2411.15410v1 Announce Type: new 
Abstract: In this paper, we present a novel approach for fluid dynamic simulations by harnessing the capabilities of Physics-Informed Neural Networks (PINNs) guided by the newly unveiled principle of minimum pressure gradient (PMPG). In a PINN formulation, the physics problem is converted into a minimization problem (typically least squares). The PMPG asserts that for incompressible flows, the total magnitude of the pressure gradient over the domain must be minimum at every time instant, turning fluid mechanics into minimization problems, making it an excellent choice for PINNs formulation. Following the PMPG, the proposed PINN formulation seeks to construct a neural network for the flow field that minimizes Nature's cost function for incompressible flows in contrast to traditional PINNs that minimize the residuals of the Navier-Stokes equations. This technique eliminates the need to train a separate pressure model, thereby reducing training time and computational costs. We demonstrate the effectiveness of this approach through a case study of inviscid flow around a cylinder, showing its ability to capture the underlying physics, while reducing computational cost and training time. The proposed approach outperforms the traditional PINNs approach in terms of Root Mean Square Error, training time, convergence rate, and compliance with physical metrics. While demonstrated on a simple geometry, the methodology is extendable to more complex flow fields (e.g., Three-Dimensional, unsteady, viscous flows) within the incompressible realm, which is the region of applicability of the PMPG.


---
# High temperature melting of dense molecular hydrogen from machine-learning interatomic potentials trained on quantum Monte Carlo

## 在量子蒙特卡罗上训练的机器学习原子间势对致密分子氢的高温熔化

Link: https://arxiv.org/abs/2411.15665

arXiv:2411.15665v1 Announce Type: new 
Abstract: We present results and discuss methods for computing the melting temperature of dense molecular hydrogen using a machine learned model trained on quantum Monte Carlo data. In this newly trained model, we emphasize the importance of accurate total energies in the training. We integrate a two phase method for estimating the melting temperature with estimates from the Clausius-Clapeyron relation to provide a more accurate melting curve from the model. We make detailed predictions of the melting temperature, solid and liquid volumes, latent heat and internal energy from 50 GPa to 180 GPa for both classical hydrogen and quantum hydrogen. At pressures of roughly 173 GPa and 1635K, we observe molecular dissociation in the liquid phase. We compare with previous simulations and experimental measurements.


---
# Can flocking aid the path planning of microswimmers in turbulent flows?

## 植绒可以帮助微流体在湍流中的路径规划吗？

Link: https://arxiv.org/abs/2411.15902

arXiv:2411.15902v1 Announce Type: new 
Abstract: We show that flocking of microswimmers in a turbulent flow can enhance the efficacy of reinforcement-learning-based path-planning of microswimmers in turbulent flows. In particular, we develop a machine-learning strategy that incorporates Vicsek-model-type flocking in microswimmer assemblies in a statistically homogeneous and isotropic turbulent flow in two dimensions (2D). We build on the adversarial-reinforcement-learning of Ref.~\cite{alageshan2020machine} for non-interacting microswimmers in turbulent flows. Such microswimmers aim to move optimally from an initial position to a target. We demonstrate that our flocking-aided version of the adversarial-reinforcement-learning strategy of Ref.~\cite{alageshan2020machine} can be superior to earlier microswimmer path-planning strategies.


---
# PINNs4Drops: Convolutional feature-enhanced physics-informed neural networks for reconstructing two-phase flows

## PINNs4Drops: 卷积特征增强的物理通知神经网络，用于重建两相流

Link: https://arxiv.org/abs/2411.15949

arXiv:2411.15949v1 Announce Type: new 
Abstract: Two-phase flow phenomena play a key role in many engineering applications, including hydrogen fuel cells, spray cooling techniques and combustion. Specialized techniques like shadowgraphy and particle image velocimetry can reveal gas-liquid interface evolution and internal velocity fields; however, they are largely limited to planar measurements, while flow dynamics are inherently three-dimensional (3D). Deep learning techniques based on convolutional neural networks provide a powerful approach for volumetric reconstruction based on the experimental data by leveraging spatial structure of images and extracting context-rich features. Building on this foundation, Physics-informed neural networks (PINNs) offer a complementary and promising alternative integrating prior knowledge in the form of governing equations into the networks training process. This integration enables accurate predictions even with limited data. By combining the strengths of both approaches, we propose a novel convolutional feature-enhanced PINNs framework, designed for the spatio-temporal reconstruction of two-phase flows from color-coded shadowgraphy images. The proposed approach is first validated on synthetic data generated through direct numerical simulation, demonstrating high spatial accuracy in reconstructing the three-dimensional gas-liquid interface, along with the inferred velocity and pressure fields. Subsequently, we apply this method to interface reconstruction for an impinging droplet using planar experimental data, highlighting the practical applicability and significant potential of the proposed approach to real-world fluid dynamics analysis.


---
# Effective Theory Building and Manifold Learning

## 有效的理论构建和流形学习

Link: https://arxiv.org/abs/2411.15975

arXiv:2411.15975v1 Announce Type: new 
Abstract: Manifold learning and effective model building are generally viewed as fundamentally different types of procedure. After all, in one we build a simplified model of the data, in the other, we construct a simplified model of the another model. Nonetheless, I argue that certain kinds of high-dimensional effective model building, and effective field theory construction in quantum field theory, can be viewed as special cases of manifold learning. I argue that this helps to shed light on all of these techniques. First, it suggests that the effective model building procedure depends upon a certain kind of algorithmic compressibility requirement. All three approaches assume that real-world systems exhibit certain redundancies, due to regularities. The use of these regularities to build simplified models is essential for scientific progress in many different domains.


---
# Coherent Ising Machine Based on Polarization Symmetry Breaking in a Driven Kerr Resonator

## 基于驱动克尔谐振器中偏振对称性破坏的相干伊辛机

Link: https://arxiv.org/abs/2411.16009

arXiv:2411.16009v1 Announce Type: new 
Abstract: Time-multiplexed networks of degenerate optical parametric oscillators have demonstrated remarkable success in simulating coupled Ising spins, thus providing a promising route to solving complex combinatorial optimization problems. In these systems $\unicode{x2014}$ referred to as coherent Ising machines $\unicode{x2014}$ the spins are encoded in the phases of the oscillators, and spin states are measured at the system output using phase-sensitive techniques. Here, we present an experimental demonstration of a conceptually new optical Ising machine based upon spontaneous polarization symmetry breaking within a coherently-driven optical fibre Kerr nonlinear resonator. In our scheme, the spin states are encoded using polarization, which allows the state of the network to be robustly read out using straightforward intensity measurements. Furthermore, by operating within a recently-discovered regime where the interplay between nonlinearity and topology fundamentally safeguards the system's symmetry, we ensure that our spins evolve without unwanted biases. This enables continuous Ising machine trials at optical data rates for up to an hour without resetting or manual adjustments. With an all-fibre implementation that relies solely on standard telecommunications components, we believe our work paves the way for substantial advances in the performance and stability of coherent optical Ising machines for applications ranging from financial modeling to drug discovery and machine learning.


---
# Label-Free Intraoperative Mean-Transition-Time Image Generation Using Statistical Gating and Deep Learning

## 使用统计门控和深度学习的无标签术中平均过渡时间图像生成

Link: https://arxiv.org/abs/2411.16039

arXiv:2411.16039v1 Announce Type: new 
Abstract: It is of paramount importance to visualize blood dynamics intraoperatively, as this enables the accurate diagnosis of intraoperative conditions and facilitates informed surgical decision-making. Indocyanine green (ICG) fluorescence imaging represents the gold standard for the assessment of blood flow and the identification of vascular structures. However, it has several disadvantages, including time-consuming data acquisition, mandatory waiting periods, potential allergic reactions, and complex operations. Laser speckle contrast imaging (LSCI) provides an alternative for label-free assessment of blood flow; however, it lacks the necessary information for distinguishing arteries from veins and determining blood flow direction. Such information may be inferred from a Mean Transition Time (MTT) image derived from fluorescence imaging. In order to address these challenges, we propose the implementation of a Mixed Attention Dense UNet (MA-DenseUNet), which will be used to generate synthetic MTT images based on statistically gated deep tissue contrast and white light images. The proposed method provides clear delineation of vasculature, differentiation of arteries and veins, decoding of blood flow direction, and a reduction in imaging time by a minimum of 97.69%. This study demonstrates the potential of deep learning to optimize intraoperative optical imaging techniques, thereby providing faster and more efficient label-free surgical guidance.


---
# Global spatio-temporal downscaling of ERA5 precipitation through generative AI

## 通过生成性AI对ERA5降水进行全球时空缩减

Link: https://arxiv.org/abs/2411.16098

arXiv:2411.16098v1 Announce Type: new 
Abstract: The spatial and temporal distribution of precipitation has a significant impact on human lives by determining freshwater resources and agricultural yield, but also rainfall-driven hazards like flooding or landslides. While the ERA5 reanalysis dataset provides consistent long-term global precipitation information that allows investigations of these impacts, it lacks the resolution to capture the high spatio-temporal variability of precipitation. ERA5 misses intense local rainfall events that are crucial drivers of devastating flooding - a critical limitation since extreme weather events become increasingly frequent. Here, we introduce spateGAN-ERA5, the first deep learning based spatio-temporal downscaling of precipitation data on a global scale. SpateGAN-ERA5 uses a conditional generative adversarial neural network (cGAN) that enhances the resolution of ERA5 precipitation data from 24 km and 1 hour to 2 km and 10 minutes, delivering high-resolution rainfall fields with realistic spatio-temporal patterns and accurate rain rate distribution including extremes. Its computational efficiency enables the generation of a large ensemble of solutions, addressing uncertainties inherent to the challenges of downscaling. Trained solely on data from Germany and validated in the US and Australia considering diverse climate zones, spateGAN-ERA5 demonstrates strong generalization indicating a robust global applicability. SpateGAN-ERA5 fulfils a critical need for high-resolution precipitation data in hydrological and meteorological research, offering new capabilities for flood risk assessment, AI-enhanced weather forecasting, and impact modelling to address climate-driven challenges worldwide.


---
# Multi-dimensional optical neural network

## 多维光学神经网络

Link: https://arxiv.org/abs/2411.16140

arXiv:2411.16140v1 Announce Type: new 
Abstract: The development of deep neural networks is witnessing fast growth in network size, which requires novel hardware computing platforms with large bandwidth and low energy consumption. Optical computing has been a potential candidate for next-generation computing systems. Specifically, wavelength-division multiplexing (WDM) has been widely adopted in optical neural network architecture to increase the computation bandwidth. Although existing WDM neural networks architectures have shown promise, they face challenges in the integration of light sources and further increase of the computing bandwidth. To overcome these issues, we introduce a mode-division multiplexing (MDM) strategy, offering a new degree of freedom in optical computing within the micro-ring resonator platform. We propose a MDM approach for small-scale networks and a multi-dimensional architecture for large-scale applications, supplementing WDM with MDM to enhance channel capacity for computations. In this work, we design and experimentally demonstrate key components for the MDM computing system, i.e., a multimode beam splitter, a thermo-optical tuner for the high-order mode, and a multimode waveguide bend. We further show a 2-by-2 matrix multiplexing system fabricated in a foundry that works for both MDM and MDM-WDM computing, which confirms that our approach successfully increases the input vector size for computing and ensures compatibility with existing WDM networks.


---
# Optoelectronic recurrent neural network using optical-electrical-optical converters with RC delay

## 使用具有RC延迟的光电光转换器的光电递归神经网络

Link: https://arxiv.org/abs/2411.16186

arXiv:2411.16186v1 Announce Type: new 
Abstract: Optical neural network (ONN) has been attracting intense attention owing to their low latency and low-power consumption. Among the ONNs, optical recurrent neural network (RNN) enables low-power and high-speed time-series data processing using a compact loop structure. The loop losses need to be efficiently compensated so that the time-series information is maintained in the RNN operation. For this purpose, we focus on the optoelectronic RNN (OE-RNN) with optical-electrical-optical (OEO) converters to compensate for the loop losses. However, the effect of resistive-capacitive (RC) delay of OEO converters on the RNN performance is unclear. Here, we study in simulation an OE-RNN equipped with OEO converters with RC delay. We confirm that our modeled OE-RNN achieves the high training accuracy of time-series data classification even when RC delay is comparably large to the time interval of time-series data. Our analyses reveal that the accumulation of time-series data by RC delay does not degrade the RNN performance but rather can compensate for the degraded RNN performance due to loop losses. From the theoretical analysis referring to the gradient explosion and vanishing problems, we find the region related to loss and RC delay where the high training accuracy can be achieved. In simulation, we confirm this compensation effect in the large OE-RNN circuit up to 32$\times$32 scale. Our proposed scheme opens a new way of time-series data processing by utilizing RC delay for the optical computing and optical communication.


---
# A constant potential reactor framework for electrochemical reaction simulations

## 用于电化学反应模拟的恒电位反应器框架

Link: https://arxiv.org/abs/2411.16330

arXiv:2411.16330v1 Announce Type: new 
Abstract: Understanding the evolution of electrified solid-liquid interfaces during electrochemical reactions is crucial. However, capturing the dynamic behavior of the interfaces with high temporal resolution and accuracy over long timescales remains a major challenge for both experimental and computational techniques. Here, we present a constant potential reactor framework that enables the simulation of electrochemical reactions with ab initio accuracy over extended timescales, allowing for real-time atomic scale observations for the electrified solid-liquid interface evolution. By implementing an enhanced sampling active learning protocol, we develop fast, accurate, and scalable neural network potentials that generalize across systems with varying electron counts, based on high-throughput density functional theory computations within an explicit-implicit hybrid solvent model. The simulation of reactions in realistic electrochemical environments uncovers the intrinsic mechanisms through which alkali metal cations promote CO2 adsorption and suppress the hydrogen evolution reaction. These findings align with previous experimental results and clarify previously elusive observations, offering valuable computational insights. Our framework lay the groundwork for future studies exploring the dynamic interplay between interfacial structure and reactivity in electrochemical environments.


---
# Comparison of Generative Learning Methods for Turbulence Modeling

## 湍流建模的生成学习方法比较

Link: https://arxiv.org/abs/2411.16417

arXiv:2411.16417v1 Announce Type: new 
Abstract: Numerical simulations of turbulent flows present significant challenges in fluid dynamics due to their complexity and high computational cost. High resolution techniques such as Direct Numerical Simulation (DNS) and Large Eddy Simulation (LES) are generally not computationally affordable, particularly for technologically relevant problems. Recent advances in machine learning, specifically in generative probabilistic models, offer promising alternatives for turbulence modeling. This paper investigates the application of three generative models - Variational Autoencoders (VAE), Deep Convolutional Generative Adversarial Networks (DCGAN), and Denoising Diffusion Probabilistic Models (DDPM) - in simulating a 2D K\'arm\'an vortex street around a fixed cylinder. Training data was obtained by means of LES. We evaluate each model's ability to capture the statistical properties and spatial structures of the turbulent flow. Our results demonstrate that DDPM and DCGAN effectively replicate the flow distribution, highlighting their potential as efficient and accurate tools for turbulence modeling. We find a strong argument for DCGAN, as although they are more difficult to train (due to problems such as mode collapse), they gave the fastest inference and training time, require less data to train compared to VAE and DDPM, and provide the results most closely aligned with the input stream. In contrast, VAE train quickly (and can generate samples quickly) but do not produce adequate results, and DDPM, whilst effective, is significantly slower at both inference and training time.


---
# Generalizable Deep Learning Approach for 3D Particle Imaging using Holographic Microscopy

## 基于全息显微镜的3D粒子成像的可推广深度学习方法

Link: https://arxiv.org/abs/2411.16439

arXiv:2411.16439v1 Announce Type: new 
Abstract: Despite its potential for label-free particle diagnostics, holographic microscopy is limited by specialized processing methods that struggle to generalize across diverse settings. We introduce a deep learning architecture leveraging human perception of longitudinal variation of diffracted patterns of particles, which enables highly generalizable analysis of 3D particle information with orders of magnitude improvement in processing speed. Trained with minimal synthetic and real holograms of simple particles, our method demonstrates exceptional performance on various challenging cases including those with high particle concentrations and noises and a wide range of particle sizes, complex shapes, and optical properties exceeding the diversity of the training datasets.


---
# Non-Linear Super-Stencils for Turbulence Model Corrections

## 用于湍流模型校正的非线性超级模板

Link: https://arxiv.org/abs/2411.16493

arXiv:2411.16493v1 Announce Type: new 
Abstract: Accurate simulation of turbulent flows remains a challenge due to the high computational cost of direct numerical simulations (DNS) and the limitations of traditional turbulence models. This paper explores a novel approach to augmenting standard models for Reynolds-Averaged Navier-Stokes (RANS) simulations using a Non-Linear Super-Stencil (NLSS). The proposed method introduces a fully connected neural network that learns a mapping from the local mean flow field to a corrective force term, which is added to a standard RANS solver in order to align its solution with high-fidelity data. A procedure is devised to extract training data from reference DNS and large eddy simulations (LES). To reduce the complexity of the non-linear mapping, the dimensionless local flow data is aligned with the local mean velocity, and the local support domain is scaled by the turbulent integral length scale. After being trained on a single periodic hill case, the NLSS-corrected RANS solver is shown to generalize to different periodic hill geometries and different Reynolds numbers, producing significantly more accurate solutions than the uncorrected RANS simulations.


---
# Reflections from the 2024 Large Language Model (LLM) Hackathon for Applications in Materials Science and Chemistry

## 2024大型语言模型 (LLM) Hackathon在材料科学和化学中的应用

Link: https://arxiv.org/abs/2411.15221

arXiv:2411.15221v1 Announce Type: cross 
Abstract: Here, we present the outcomes from the second Large Language Model (LLM) Hackathon for Applications in Materials Science and Chemistry, which engaged participants across global hybrid locations, resulting in 34 team submissions. The submissions spanned seven key application areas and demonstrated the diverse utility of LLMs for applications in (1) molecular and material property prediction; (2) molecular and material design; (3) automation and novel interfaces; (4) scientific communication and education; (5) research data management and automation; (6) hypothesis generation and evaluation; and (7) knowledge extraction and reasoning from scientific literature. Each team submission is presented in a summary table with links to the code and as brief papers in the appendix. Beyond team results, we discuss the hackathon event and its hybrid format, which included physical hubs in Toronto, Montreal, San Francisco, Berlin, Lausanne, and Tokyo, alongside a global online hub to enable local and virtual collaboration. Overall, the event highlighted significant improvements in LLM capabilities since the previous year's hackathon, suggesting continued expansion of LLMs for applications in materials science and chemistry research. These outcomes demonstrate the dual utility of LLMs as both multipurpose models for diverse machine learning tasks and platforms for rapid prototyping custom applications in scientific research.


---
# Boosting Photon-Number-Resolved Detection Rates of Transition-Edge Sensors by Machine Learning

## 通过机器学习提高跃迁边缘传感器的光子数分辨检测率

Link: https://arxiv.org/abs/2411.15360

arXiv:2411.15360v1 Announce Type: cross 
Abstract: Transition-Edge Sensors (TESs) are very effective photon-number-resolving (PNR) detectors that have enabled many photonic quantum technologies. However, their relatively slow thermal recovery time severely limits their operation rate in experimental scenarios compared to leading non-PNR detectors. In this work, we develop an algorithmic approach that enables TESs to detect and accurately classify photon pulses without waiting for a full recovery time between detection events. We propose two machine-learning-based signal processing methods: one supervised learning method and one unsupervised clustering method. By benchmarking against data obtained using coherent states and squeezed states, we show that the methods extend the TES operation rate to 800 kHz, achieving at least a four-fold improvement, whilst maintaining accurate photon-number assignment up to at least five photons. Our algorithms will find utility in applications where high rates of PNR detection are required and in technologies which demand fast active feed-forward of PNR detection outcomes.


---
# Improved Initial Guesses for Numerical Solutions of Kepler's Equation

## 改进的开普勒方程数值解的初始猜测

Link: https://arxiv.org/abs/2411.15374

arXiv:2411.15374v1 Announce Type: cross 
Abstract: Numerical solutions of Kepler's Equation are critical components of celestial mechanics software, and are often computation hot spots. This work uses symbolic regression and a genetic learning algorithm to find new initial guesses for iterative Kepler solvers for both elliptical and hyperbolic orbits. The new initial guesses are simple to implement, and result in modest speed improvements for elliptical orbits, and major speed improvements for hyperbolic orbits.


---
# Enhancing Open Quantum Dynamics Simulations Using Neural Network-Based Non-Markovian Stochastic Schr\"odinger Equation Method

## 使用基于神经网络的非马尔可夫随机Schr \ “odinger方程方法增强开放量子动力学模拟

Link: https://arxiv.org/abs/2411.15914

arXiv:2411.15914v1 Announce Type: cross 
Abstract: The Non-Markovian Stochastic Schrodinger Equation (NMSSE) offers a promising approach for open quantum simulations, especially in large systems, owing to its low scaling complexity and suitability for parallel computing. However, its application at low temperatures faces significant convergence challenges. While short-time evolution converges quickly, long-time evolution requires a much larger number of stochastic trajectories, leading to high computational costs. To this end,we propose a scheme that combines neural network techniques with simulations of the non-Markovian stochastic Schrodinger equation. By integrating convolutional neural networks (CNNs) and long short-term memory recurrent neural networks (LSTMs),along with the iterative attentional feature fusion (iAFF) technique, this approach significantly reduces the number of trajectories required for long-time simulations, particularly at low temperatures, thereby substantially lowering computational costs and improving convergence. To demonstrate our approach, we investigated the dynamics of the spin-boson model and the Fenna-Matthews-Olson (FMO) complex across a range of parameter variations.


---
# Understanding Machine Learning Paradigms through the Lens of Statistical Thermodynamics: A tutorial

## 通过统计热力学的镜头理解机器学习范例: 教程

Link: https://arxiv.org/abs/2411.15945

arXiv:2411.15945v1 Announce Type: cross 
Abstract: This tutorial investigates the convergence of statistical mechanics and learning theory, elucidating the potential enhancements in machine learning methodologies through the integration of foundational principles from physics. The tutorial delves into advanced techniques like entropy, free energy, and variational inference which are utilized in machine learning, illustrating their significant contributions to model efficiency and robustness. By bridging these scientific disciplines, we aspire to inspire newer methodologies in researches, demonstrating how an in-depth comprehension of physical systems' behavior can yield more effective and dependable machine learning models, particularly in contexts characterized by uncertainty.


---
# VICON: Vision In-Context Operator Networks for Multi-Physics Fluid Dynamics Prediction

## VICON: 用于多物理场流体动力学预测的视觉上下文操作员网络

Link: https://arxiv.org/abs/2411.16063

arXiv:2411.16063v1 Announce Type: cross 
Abstract: In-Context Operator Networks (ICONs) are models that learn operators across different types of PDEs using a few-shot, in-context approach. Although they show successful generalization to various PDEs, existing methods treat each data point as a single token, and suffer from computational inefficiency when processing dense data, limiting their application in higher spatial dimensions. In this work, we propose Vision In-Context Operator Networks (VICON), incorporating a vision transformer architecture that efficiently processes 2D functions through patch-wise operations. We evaluated our method on three fluid dynamics datasets, demonstrating both superior performance (reducing scaled $L^2$ error by $40\%$ and $61.6\%$ for two benchmark datasets for compressible flows, respectively) and computational efficiency (requiring only one-third of the inference time per frame) in long-term rollout predictions compared to the current state-of-the-art sequence-to-sequence model with fixed timestep prediction: Multiple Physics Pretraining (MPP). Compared to MPP, our method preserves the benefits of in-context operator learning, enabling flexible context formation when dealing with insufficient frame counts or varying timestep values.


---
# You only thermoelastically deform once: Point Absorber Detection in LIGO Test Masses with YOLO

## 您只能热弹性变形一次: 使用YOLO在lgo测试一下质量中进行点吸收体检测

Link: https://arxiv.org/abs/2411.16104

arXiv:2411.16104v1 Announce Type: cross 
Abstract: Current and future gravitational-wave observatories rely on large-scale, precision interferometers to detect the gravitational-wave signals. However, microscopic imperfections on the test masses, known as point absorbers, cause problematic heating of the optic via absorption of the high-power laser beam, which results in diminished sensitivity, lock loss, or even permanent damage. Consistent monitoring of the test masses is crucial for detecting, characterizing, and ultimately removing point absorbers. We present a machine-learning algorithm for detecting point absorbers based on the object-detection algorithm You Only Look Once (YOLO). The algorithm can perform this task in situ while the detector is in operation. We validate our algorithm by comparing it with past reports of point absorbers identified by humans at LIGO. The algorithm confidently identifies the same point absorbers as humans with minimal false positives. It also identifies some point absorbers previously not identified by humans, which we confirm with human follow-up. We highlight the potential of machine learning in commissioning efforts.


---
# Ultrahigh-fidelity spatial mode quantum gates in high-dimensional space by diffractive deep neural networks

## 基于衍射深度神经网络的高维空间超高保真空间模式量子门

Link: https://arxiv.org/abs/2411.16410

arXiv:2411.16410v1 Announce Type: cross 
Abstract: While the spatial mode of photons is widely used in quantum cryptography, its potential for quantum computation remains largely unexplored. Here, we showcase the use of the multi-dimensional spatial mode of photons to construct a series of high-dimensional quantum gates, achieved through the use of diffractive deep neural networks (D2NNs). Notably, our gates demonstrate high fidelity of up to 99.6(2)%, as characterized by quantum process tomography. Our experimental implementation of these gates involves a programmable array of phase layers in a compact and scalable device, capable of performing complex operations or even quantum circuits. We also demonstrate the efficacy of the D2NN gates by successfully implementing the Deutsch algorithm and propose an intelligent deployment protocol that involves self-configuration and self-optimization. Moreover, we conduct a comparative analysis of the D2NN gate's performance to the wave-front matching approach. Overall, our work opens a door for designing specific quantum gates using deep learning, with the potential for reliable execution of quantum computation.


---
# Generalizable data-driven turbulence closure modeling on unstructured grids with differentiable physics

## 可微分物理的非结构化网格上可推广的数据驱动湍流闭合建模

Link: https://arxiv.org/abs/2307.13533

arXiv:2307.13533v2 Announce Type: replace 
Abstract: Differentiable physical simulators are proving to be valuable tools for developing data-driven models in computational fluid dynamics (CFD). These simulators enable end-to-end training of machine learning (ML) models embedded within CFD solvers. This paradigm enables novel algorithms which combine the generalization power and low cost of physics-based simulations with the flexibility and automation of deep learning methods. In this study, we introduce a framework for embedding deep learning models within a generic finite element solver to solve the Navier-Stokes equations, specifically applying this approach to learn a subgrid scale closure with a graph neural network (GNN). We validate our method for flow over a backwards-facing step and test its performance on novel geometries, demonstrating the ability to generalize to novel geometries without sacrificing stability. Additionally, we show that our GNN-based closure model may be learned in a data-limited scenario by interpreting closure modeling as a solver-constrained optimization. Our end-to-end learning paradigm demonstrates a viable pathway for physically consistent and generalizable data-driven closure modeling across complex geometries.


---
# Interpretable machine learning approach for electron antineutrino selection in a large liquid scintillator detector

## 大型液体闪烁体探测器中电子抗中离子选择的可解释机器学习方法

Link: https://arxiv.org/abs/2406.12901

arXiv:2406.12901v2 Announce Type: replace 
Abstract: Several neutrino detectors, KamLAND, Daya Bay, Double Chooz, RENO, and the forthcoming large-scale JUNO, rely on liquid scintillator to detect reactor antineutrino interactions. In this context, inverse beta decay represents the golden channel for antineutrino detection, providing a pair of correlated events, thus a strong experimental signature to distinguish the signal from a variety of backgrounds. However, given the low cross-section of antineutrino interactions, the development of a powerful event selection algorithm becomes imperative to achieve effective discrimination between signal and backgrounds. In this study, we introduce a machine learning (ML) model to achieve this goal: a fully connected neural network as a powerful signal-background discriminator for a large liquid scintillator detector. We demonstrate, using the JUNO detector as an example, that, despite the already high efficiency of a cut-based approach, the presented ML model can further improve the overall event selection efficiency. Moreover, it allows for the retention of signal events at the detector edges that would otherwise be rejected because of the overwhelming amount of background events in that region. We also present the first interpretable analysis of the ML approach for event selection in reactor neutrino experiments. This method provides insights into the decision-making process of the model and offers valuable information for improving and updating traditional event selection approaches.


---
# Learning thresholds lead to stable language coexistence

## 学习阈值导致稳定的语言共存

Link: https://arxiv.org/abs/2406.14522

arXiv:2406.14522v2 Announce Type: replace 
Abstract: We introduce a language competition model that is based on the Abrams-Strogatz model and incorporates the effects of memory and learning in the language shift dynamics. On a coarse grained time scale, the effects of memory and learning can be expressed as thresholds on the speakers fractions of the competing languages. In its simplest form, the resulting model is exactly solvable. Besides the consensus on one of the two languages, the model describes additional equilibrium states that are not present in the Abrams-Strogatz model: a stable dynamical coexistence of the two languages and a frozen state coinciding with the initial state. We show numerically that these results are preserved for threshold functions of a more general shape. The comparison of the model predictions with historical datasets demonstrates that while the Abrams-Strogatz model fails to describe some relevant language competition situations, the proposed model provides a good fitting.


---
# Multi-agent reinforcement learning for the control of three-dimensional Rayleigh-B\'enard convection

## 控制三维rayleigh-b \ &#39;enard对流的多智能体强化学习

Link: https://arxiv.org/abs/2407.21565

arXiv:2407.21565v2 Announce Type: replace 
Abstract: Deep reinforcement learning (DRL) has found application in numerous use-cases pertaining to flow control. Multi-agent RL (MARL), a variant of DRL, has shown to be more effective than single-agent RL in controlling flows exhibiting locality and translational invariance. We present, for the first time, an implementation of MARL-based control of three-dimensional Rayleigh-B\'enard convection (RBC). Control is executed by modifying the temperature distribution along the bottom wall divided into multiple control segments, each of which acts as an independent agent. Two regimes of RBC are considered at Rayleigh numbers $\mathrm{Ra}=500$ and $750$. Evaluation of the learned control policy reveals a reduction in convection intensity by $23.5\%$ and $8.7\%$ at $\mathrm{Ra}=500$ and $750$, respectively. The MARL controller converts irregularly shaped convective patterns to regular straight rolls with lower convection that resemble flow in a relatively more stable regime. We draw comparisons with proportional control at both $\mathrm{Ra}$ and show that MARL is able to outperform the proportional controller. The learned control strategy is complex, featuring different non-linear segment-wise actuator delays and actuation magnitudes. We also perform successful evaluations on a larger domain than used for training, demonstrating that the invariant property of MARL allows direct transfer of the learnt policy.


---
# Single-snapshot machine learning for super-resolution of turbulence

## 用于湍流超分辨率的单快照机器学习

Link: https://arxiv.org/abs/2409.04923

arXiv:2409.04923v2 Announce Type: replace 
Abstract: Modern machine-learning techniques are generally considered data-hungry. However, this may not be the case for turbulence as each of its snapshots can hold more information than a single data file in general machine-learning settings. This study asks the question of whether nonlinear machine-learning techniques can effectively extract physical insights even from as little as a {\it single} snapshot of turbulent flow. As an example, we consider machine-learning-based super-resolution analysis that reconstructs a high-resolution field from low-resolution data for two examples of two-dimensional isotropic turbulence and three-dimensional turbulent channel flow. First, we reveal that a carefully designed machine-learning model trained with flow tiles sampled from only a single snapshot can reconstruct vortical structures across a range of Reynolds numbers for two-dimensional decaying turbulence. Successful flow reconstruction indicates that nonlinear machine-learning techniques can leverage scale-invariance properties to learn turbulent flows. We also show that training data of turbulent flows can be cleverly collected from a single snapshot by considering characteristics of rotation and shear tensors. Second, we perform the single-snapshot super-resolution analysis for turbulent channel flow, showing that it is possible to extract physical insights from a single flow snapshot even with inhomogeneity. The present findings suggest that embedding prior knowledge in designing a model and collecting data is important for a range of data-driven analyses for turbulent flows. More broadly, this work hopes to stop machine-learning practitioners from being wasteful with turbulent flow data.


---
# dCG -- differentiable connected geometries for AI-compatible multi-domain optimization and inverse design

## dCG-AI兼容多域优化和逆设计的可微连接几何

Link: https://arxiv.org/abs/2410.05833

arXiv:2410.05833v3 Announce Type: replace 
Abstract: In the domain of geometry and topology optimization, discovering geometries that optimally satisfy specific problem criteria is a complex challenge in both engineering and scientific research. In this work, we propose a new approach for the creation of multidomain connected geometries that are designed to work with automatic differentiation. We introduce the concept of differentiable Connected Geometries (dCG), discussing its theoretical aspects and illustrating its application through a simple toy examples and a more sophisticated photonic optimization task. Since these geometries are built upon the principles of automatic differentiation, they are compatible with existing deep learning frameworks, a feature we demonstrate via the application examples. This methodology provides a systematic way to approach geometric design and optimization in computational fields involving dependent geometries, potentially improving the efficiency and effectiveness of optimization tasks in scientific and engineering applications.


---
# Performance of the CMS high-level trigger during LHC Run 2

## LHC运行2期间CMS高级触发器的性能

Link: https://arxiv.org/abs/2410.17038

arXiv:2410.17038v2 Announce Type: replace 
Abstract: The CERN LHC provided proton and heavy ion collisions during its Run 2 operation period from 2015 to 2018. Proton-proton collisions reached a peak instantaneous luminosity of 2.1 $\times$ 10$^{34}$ cm$^{-2}$s$^{-1}$, twice the initial design value, at $\sqrt{s}$ = 13 TeV. The CMS experiment records a subset of the collisions for further processing as part of its online selection of data for physics analyses, using a two-level trigger system: the Level-1 trigger, implemented in custom-designed electronics, and the high-level trigger, a streamlined version of the offline reconstruction software running on a large computer farm. This paper presents the performance of the CMS high-level trigger system during LHC Run 2 for physics objects, such as leptons, jets, and missing transverse momentum, which meet the broad needs of the CMS physics program and the challenge of the evolving LHC and detector conditions. Sophisticated algorithms that were originally used in offline reconstruction were deployed online. Highlights include a machine-learning b tagging algorithm and a reconstruction algorithm for tau leptons that decay hadronically.


---
# Realization of High-Fidelity CZ Gate based on a Double-Transmon Coupler

## 基于双传输耦合器的高保真CZ门的实现

Link: https://arxiv.org/abs/2402.18926

arXiv:2402.18926v2 Announce Type: replace-cross 
Abstract: Striving for higher gate fidelity is crucial not only for enhancing existing noisy intermediate-scale quantum (NISQ) devices but also for unleashing the potential of fault-tolerant quantum computation through quantum error correction. A recently proposed theoretical scheme, the double-transmon coupler (DTC), aims to achieve both suppressed residual interaction and a fast high-fidelity two-qubit gate simultaneously, particularly for highly detuned qubits. Harnessing the state-of-the-art fabrication techniques and a model-free pulse-optimization process based on reinforcement learning, we translate the theoretical DTC scheme into reality, attaining fidelities of 99.92% for a CZ gate and 99.98% for single-qubit gates. The performance of the DTC scheme demonstrates its potential as a competitive building block for superconducting quantum processors.


---
# A probabilistic framework for learning non-intrusive corrections to long-time climate simulations from short-time training data

## 从短期训练数据学习对长期气候模拟的非侵入性校正的概率框架

Link: https://arxiv.org/abs/2408.02688

arXiv:2408.02688v2 Announce Type: replace-cross 
Abstract: Chaotic systems, such as turbulent flows, are ubiquitous in science and engineering. However, their study remains a challenge due to the large range scales, and the strong interaction with other, often not fully understood, physics. As a consequence, the spatiotemporal resolution required for accurate simulation of these systems is typically computationally infeasible, particularly for applications of long-term risk assessment, such as the quantification of extreme weather risk due to climate change. While data-driven modeling offers some promise of alleviating these obstacles, the scarcity of high-quality simulations results in limited available data to train such models, which is often compounded by the lack of stability for long-horizon simulations. As such, the computational, algorithmic, and data restrictions generally imply that the probability of rare extreme events is not accurately captured. In this work we present a general strategy for training neural network models to non-intrusively correct under-resolved long-time simulations of chaotic systems. The approach is based on training a post-processing correction operator on under-resolved simulations nudged towards a high-fidelity reference. This enables us to learn the dynamics of the underlying system directly, which allows us to use very little training data, even when the statistics thereof are far from converged. Additionally, through the use of probabilistic network architectures we are able to leverage the uncertainty due to the limited training data to further improve extrapolation capabilities. We apply our framework to severely under-resolved simulations of quasi-geostrophic flow and demonstrate its ability to accurately predict the anisotropic statistics over time horizons more than 30 times longer than the data seen in training.


---
# Machine learned reconstruction of tsunami dynamics from sparse observations

## 从稀疏观测中机器学习重建海啸动力学

Link: https://arxiv.org/abs/2411.12948

arXiv:2411.12948v2 Announce Type: replace-cross 
Abstract: We investigate the use of the Senseiver, a transformer neural network designed for sparse sensing applications, to estimate full-field surface height measurements of tsunami waves from sparse observations. The model is trained on a large ensemble of simulated data generated via a shallow water equations solver, which we show to be a faithful reproduction for the underlying dynamics by comparison to historical events. We train the model on a dataset consisting of 8 tsunami simulations whose epicenters correspond to historical USGS earthquake records, and where the model inputs are restricted to measurements obtained at actively deployed buoy locations. We test the Senseiver on a dataset consisting of 8 simulations not included in training, demonstrating its capability for extrapolation. The results show remarkable resolution of fine scale phase and amplitude features from the true field, provided that at least a few of the sensors have obtained a non-zero signal. Throughout, we discuss which forecasting techniques can be improved by this method, and suggest ways in which the flexibility of the architecture can be leveraged to incorporate arbitrary remote sensing data (eg. HF Radar and satellite measurements) as well as investigate optimal sensor placements.


---
# Discriminant Analysis Optimizes Progress Coordinate in Weighted Ensemble Simulations of Rare Event Kinetics

## 判别分析优化稀有事件动力学加权集合模拟中的进度坐标

Link: https://dx.doi.org/10.26434/chemrxiv-2024-l8tpq?rft_dat=source%3Ddrss

Calculating the kinetics of rare-but-important conformational transitions in complex biomolecules is a significant challenge in computational biophysics. Because of the long timescales needed to observe such processes, regular molecular dynamics simulations are too slow to sample these events by direct integration of the equations of motion. Recently, the weighted ensemble method has gained significant popularity for its ability to compute the rates of conformational transitions in biomolecular systems using unbiased simulations. However, the progress coordinate(s) of the weighted ensemble simulation should be carefully designed to capture the slow degrees of freedom of the system. Here, we demonstrate the application of a machine learning approach, harmonic linear discriminant analysis, which builds a predictive model for class membership, to design progress coordinates for weighted ensemble simulations. We test the accuracy and efficiency of this technique for computing the kinetics of the conformational transition of alanine dipeptide and the unfolding of a small protein. The key advantage of our data-driven approach is its minimal system knowledge requirement, which potentially extends its applicability to more complex and physiologically relevant systems.


---
# Optimizing Sentiment Classification Using Dynamic Weighted Stacking Ensemble of Pre-trained Models

## 使用预训练模型的动态加权堆叠集成优化情感分类

Link: https://www.researchsquare.com/article/rs-5408406/latest

In the era of information explosion, sentiment classification, as a crucial task in natural language processing, is widely applied across various fields, including e-commerce, media, government, and finance. This paper proposes a dynamic weighted stacking ensemble method, integrating three pre-trained models: NeZha, XLNet, and ERNIE.The approach leverages Bayesian optimization to dynamically adjust the weights of the models. Therefore, robustness and generalization in sentiment classification tasks are improved. Experiments conducted on the SMP2020 Weibo sentiment classification dataset and the ChnSentiCorp sentiment analysis dataset confirm the effectiveness of this method. Results show that the ensemble model significantly outperforms individual models and traditional ensemble methods in terms of classification accuracy and F1 score, particularly excelling in handling complex emotional expressions. This study provides a new solution for sentiment classification tasks and demonstrates the potential of ensemble learning to enhance model performance .


---
# Enhancing Multi-View Deep Image Clustering via Contrastive Learning for Global and Local Consistency

## 通过对比学习增强多视图深度图像聚类的全局和局部一致性

Link: https://www.researchsquare.com/article/rs-5407388/latest

Multi-view clustering (MVC) is a data clustering method with many applications, including but not limited to image and video analysis, text and language processing, bioinformatics, and signal processing. The objective of multi-view deep clustering is to enhance the efficacy of clustering algorithms by integrating data from disparate views. However, discrepancies and inconsistencies between different views frequently reduce the precision of the clustering outcomes. In the recent popular comparative learning, it has been observed that the processing of positive and negative samples does not consider the multi-view consistency information, ultimately resulting in a decline in clustering accuracy. In this paper, we put forth a global and local consistency-based contrast learning framework to enhance the efficacy of multi-view deep clustering. First, a global consistency constraint is designed to ensure that the global representations of different views can be aligned to capture the data's main features. Secondly, we introduce a local consistency mechanism, which aims to preserve the unique local information in each view and obtain efficient, positive samples to improve the complementarity and robustness of the inter-view representations through comparative learning. The experimental results demonstrate that the proposed method markedly enhances the clustering performance on several real benchmark datasets, mainly when dealing with multi-view data with incompleteness.


---
# Virtual Screening of Antivirals Targeting H275Y Mutation in Neuraminidase gene of Oseltamivir Drug Resistant Influenza Strains

## 针对奥司他韦耐药流感病毒株神经氨酸酶基因H275Y突变的抗病毒药物虚拟筛选

Link: https://www.researchsquare.com/article/rs-5211962/latest

Neuraminidase (NA) is an essential enzyme located at the outer layer of the influenza virus and plays a key role in the release of virions. This study aims to identify and analyze possible inhibitors of NA from different subtypes of influenza viruses, especially considering global outbreaks that highlight the immediate requirement for effective antiviral drugs. Initially, a thorough search was conducted in the Protein Data Bank (PDB) to gather structures of NA proteins that were attached with oseltamivir, a widely recognized inhibitor of NA. Here, 36 PDB entries were found with NA-oseltamivir complexes which were studied to evaluate the diversity and mutations present in various subtypes. Finally, N1(H1N1) protein was selected that demonstrated low IC50 value of oseltamivir with mutation H275Y. In addition, the study utilized BiMODAL generative model to generate 1000 novel molecules with comparable structures to oseltamivir. A QSAR model, based on machine learning (ML), was built utilizing the ChEMBL database to improve the selection process of candidate inhibitors. These inhibitors were subsequently analyzed by molecular docking and further the best hits compounds (compound_375, compound_106 and compound_597) were tacked to make a bigger molecule (compound_106&amp;ndash;375, compound_106&amp;ndash;597, and compound_375&amp;ndash;597) to fit into the binding pocket of protein. Further, triplicate molecular dynamics simulations lasting 100 ns to assess their effectiveness and binding stability, showed that compound_106&amp;ndash;375 had the most stable binding with the protein. Key residues, including Asn146, Ala138, and Tyr155, form critical interactions with the ligand, contributing to its stability. The investigation was enhanced by employing principal component analysis (PCA), free energy landscape (FEL), and binding free energy calculations. The total binding free energy (GTOTAL) of -17.74 kcal/mol suggests that the contact between compound_106&amp;ndash;375 and the mutant N1 (H1N1) protein is thermodynamically favorable. This approach allowed for a thorough comprehension of the binding interactions and possible effectiveness of the discovered inhibitors. Overall, these findings demonstrate that compound_106&amp;ndash;375 exhibits favorable binding characteristics and stability, making it a promising candidate for further development as a therapeutic agent against the mutant N1 (H1N1) protein, potentially overcoming the challenges associated with drug resistance in influenza viruses.


---
# Development and Validation of a Prediction Model for Coronary Artery Disease in Chest Pain Patients: A Real-World Multicenter Study Based on Machine Learning

## 胸痛患者冠状动脉疾病预测模型的开发和验证: 基于机器学习的真实世界多中心研究

Link: https://www.researchsquare.com/article/rs-5234204/latest

Coronary artery disease (CAD) is a prevalent condition among chest pain patients, and accurate prediction of the disease is crucial to ensure timely interventions and improve patient outcomes. We aim to elaborate a prediction model for CAD in chest pain patients using machine learning approaches. A retrospective analysis was performed using electronic health records of patients who presented with chest pain at seven hospitals. A total of 8474 patients were included in the study, where 63.25% were diagnosed with CAD. The data included demographic information, medical history, and laboratory results. Machine learning algorithms, including Random Forest, CatBoost, XGBoosting, Gradient Boosting, Light Gradient, AdaBoost, Ridge Classifier, Linear Discriminant, Logistic Regression, Decision Tree, SVM, Quadratic Discriminant, K Neighbors, Naive Bayes, and Dummy Classifier were trained and evaluated to predict the presence of CAD.The prediction model achieved an overall accuracy of 0.766 in identifying CAD in chest pain patients. The sensitivity and precision were 0.938 and 0.746, respectively. Important predictors for CAD included age, pulse rate, monocyte, and red cell distribution width SD. The eXtreme Gradient Boosting showed the best performance (area under the receiver operating characteristics, AUROC, 0.820, and 95% CI, 0.801&amp;ndash;0.839) Additionally, the model demonstrated robust performance in the validation group. This study successfully developed and validated a prediction model for CAD in chest pain patients using machine learning techniques. The model exhibited good predictive ability and could aid in the early identification of CAD in clinical practice, potentially leading to appropriate interventions and improved patient outcomes.


---
# Development of an intelligent metal forming robot and application to multi-stage Cold Forging

## 智能金属成形机器人的研制及其在多阶段冷锻中的应用

Link: https://www.researchsquare.com/article/rs-5109889/latest

Metal forming processes often undergo different instability phases due to different factors like tool and part temperature variations, tool vibrations or frictional interactions between workpiece and tool. In the absence of an experienced process operator, these instabilities can induce a very considerable production loss. This study addresses this issue and proposes a method to develop a data-based virtual process operator equipped with the appropriate hardware and physical components that allow it to constantly monitor and if necessary regulate the process. The resulting system is introduced as the intelligent metal forming robot. The objective of this self-learning system is first to stabilize the process and ensure a certain part quality despite the noises, dynamical disturbances and user-defined changes of the part quality requirements, then, to control the process even in states that have not yet been experienced and at last to improve the control precision based on the updated process experience. This intelligent metal forming robot has been implemented and applied on a two-stage cold forging process, where the target quality feature was the part head height of a screw-like part. The results showed that, based on a qualitative process experience and on effective actuators, an intelligent self-learning system can significantly increase the robustness of a metal forming process.


---
# Research on Rock Strength Prediction Model Based on Machine Learning Algorithm

## 基于机器学习算法的岩石强度预测模型研究

Link: https://www.researchsquare.com/article/rs-5049103/latest

The compressive strength of rocks is one of its mechanical characteristics. It has been a difficult problem to predict rock compressive strength conveniently and efficiently, and to solve the limitations of traditional rock compressive strength tests such as high cost, long time consumption, and reliability assurance. In this study, a data set containing 1774 groups of rock compressive strength test data was constructed through file retrieval, including 9 input parameters: rock type, temperature, confining pressure, dimension of specimen, shape of specimen, and experimental method. Eight supervised learning algorithms were used to learn the rock compressive strength test data, and eight rock compressive strength prediction models considering multiple factors were established to obtain a better method of predicting rock compressive strength. By selecting different features, the optimal feature combination for predicting rock compressive strength was obtained, and the optimal parameters for different models were obtained through the Sparrow Search Algorithm (SSA). Finally, four regression evaluation indicators, including mean absolute error (MAE), root mean square error (RMSE), mean absolute percentage error (MAPE), and coefficient of determination (R&amp;sup2;), were used to evaluate the predictive performance of the established regression models. The results showed that the best-trained model had a MAPE as low as 3.61%, MAE as low as 9.19 MPa, and R&amp;sup2; as high as 0.995. It is noteworthy that AdaBoost was found to be the best model for predicting rock compressive strength. This study presents a significant advancement in the field by demonstrating the effectiveness of machine learning algorithms in this context, which have not been extensively applied to rock compressive strength predictions. The findings suggest that these models can offer substantial improvements over traditional methods, not only in accuracy but also in operational efficiency. This research is important for geotechnical engineering, as accurate rock strength predictions are critical for the design and stability assessments of construction projects, ultimately contributing to safer and more cost-effective engineering solutions.


---
# Application of Mini-CEX combined with student-standardized patient in rehabilitation medicine residents&rsquo; standardized training

## Mini-cex联合学生标准化病人在康复医学住院医师规范化培训中的应用

Link: https://www.researchsquare.com/article/rs-5407012/latest

Background: The mini clinical evaluation exercise (Mini-CEX) is widely recognized as an effective measure for standardized training for resident physicians (RST) in European and American countries. However, in China, the Mini-CEX evaluation is limited to clinical patients. No previous studies have examined the effectiveness and correct methods of Mini-CEX, combined with student-standardized patient (SSP) evaluation, in rehabilitation medicine education. Therefore, this study aimed to explore whether combining Mini-CEX and SSP can improve resident physicians&rsquo; clinical and communication skills.&amp;nbsp;
Methods: The sample comprised students who received standardized training for resident physicians from January 2022 to December 2023. Before the training began, the observation and control groups underwent Mini-CEX evaluation. During the training period, the observation group underwent Mini-CEX combined with SSP evaluation every two weeks, while the control group was not evaluated. Before leaving the department after the eighth week, the resident physicians&rsquo; learning outcomes were uniformly evaluated, including theoretical knowledge, Mini-CEX, clinical thinking ability, and specialized skill operation ability evaluation. A questionnaire survey method was adopted to collect students&rsquo; evaluations of teaching.&amp;nbsp;
Results: The Mini-CEX combined with the SSP group showed a significant improvement in resident physicians&rsquo; comprehensive clinical ability&amp;nbsp;(p&amp;lt;0.05). Compared with traditional Mini-CEX, resident physicians made greater progress in clinical thinking and patient communication skills&amp;nbsp;(p&amp;lt;0.05). The survey results indicated that resident physicians have a high level of satisfaction with the evaluation of Mini-CEX combined with SSP.&amp;nbsp;
Conclusion: Mini-CEX combined with SSP is an effective and reliable evaluation method for rehabilitation resident physician training, which fully emphasizes its advantages in improving students&rsquo; clinical abilities.


---
# Boosting Commit Classification with Contrastive Learning

## 用对比学习促进提交分类

Link: https://www.researchsquare.com/article/rs-5412995/latest

Commit Classification (CC) is an important task in software maintenance, which helps software developers classify commit changes into different types according to their nature and purpose. However, existing models need lots of manually labeled data for model fine-tuning, when training samples are insufficient, ensuring the performance of commit classification becomes very critical. The scarcity of data also leads to the problem of poor model generalization ability, resulting in satisfactory performance only on specific tasks. Moreover, they often ignore the sentence-level semantic information in the commit message, which is essential for discovering the difference between diverse commits, especially for fewshot scenarios. In this work, we propose to boost commit classification with contrastive learning. This method can solve the CC problem in fewshot scenarios. To augment the training datasets and improve the generalization ability of our proposed method, we generate additional training samples by Semantic Prototype, which is defined as a representative embedding for a group of semantically similar instances. To produce meaningful and discriminating sentence-level vectors for each commit in a pair, we employ a pretrained Sentence-Transformer as the embedding layer. The network then learns to maximize the distance in the latent space for positive pairs and minimize it for negative pairs, leading to a fine-tuned Sentence-Transformer with fixed weights for the downstream commit classification task. Extensive experiments on two open available datasets demonstrate that our framework, though simple, can solve the CC problem effectively even in fewshot scenarios. It not only achieves state-of-the-art performance but also improves the adaptability of the model without requiring a large number of training samples for fine-tuning. The code, data, and trained models are available at https://github.com/CUMT-GMSC/CommitFit.


---
# Adolescents' understanding of pain and their preferences for learning about pain at school: a cross-sectional survey.

## 青少年对疼痛的理解及其在学校学习疼痛的偏好: 一项横断面调查。

Link: https://www.researchsquare.com/article/rs-5383864/latest

Introduction: Pain contributes significant societal and economic burden, yet there are no well-resourced public education initiatives providing information about pain to the general population. Adolescence is an opportune time to deliver this information. Pain is prevalent in young people, and impacts their physical, social, and psychological health during adolescence and potentially throughout their life. Delivering pain education in schools ensures all adolescents have access to evidence-based information about pain, which could change the trajectory of pain across the lifespan. Few studies have delivered pain education to schoolchildren, and none have considered their preferences for learning about pain. We aimed to identify adolescents&amp;rsquo; understanding of pain and their preferences for learning about pain in school to inform the development of a school-based pain education module.Methods: We conducted an online cross-sectional survey of 501 Australian students in grades 7&amp;ndash;10 (mean age&amp;thinsp;=&amp;thinsp;14 [SD 1.3]; 50.9% male) about their knowledge of pain using the Conceptualization of Pain Questionnaire, and preferences for pain education content and delivery mode when learning about pain at school.Results: On average, participants got approximately half (7.4) of the fifteen pain knowledge items correct. Participants preferred to learn about ways to treat pain (70.5%), different types of pain (67.1%) and the role of the brain and nerves in pain (62.1%).  Most preferred to be taught about pain by health professionals (84.4%), followed by teachers (48.3%), through the combination educational video and class discussion (67.6%).Conclusions: Adolescents have some beliefs about pain and injury that do not align with current scientific understanding of pain. These misconceptions could be addressed in pain education at school. School-based pain education programs should include content that aligns with their preferences and use a combination of online and in-person approaches to engage adolescents.


---
# Development of Arduino-based Electronic Component Testing Device

## 基于Arduino的电子元器件测试装置的研制

Link: https://www.researchsquare.com/article/rs-5507569/latest

As advancements in electronics continued, the limitations of traditional component testers became increasingly evident. Most available testers were designed to evaluate a single component type, resulting in a circuit testing process that was often cumbersome and time-consuming. This inefficiency posed challenges to both learning and innovation in educational and professional environments. To address the growing demand for more efficient and versatile testing tools, the development of a multi-component tester became crucial. The proposed project aimed to create an integrated electronic components tester capable of evaluating a wide range of components, including integrated circuits (ICs), resistors, transistors, and capacitors. This all-in-one device was designed to streamline the testing process, improving the user experience in electronics laboratories and supporting professionals in the fields of electronics repair and development.


---
# The impact of genotype transformations and aneuploidy on genomic prediction accuracy in polyploid species: a simulation study

## 基因型转换和非整倍性对多倍体物种基因组预测准确性的影响: 一项模拟研究

Link: https://www.researchsquare.com/article/rs-5365958/latest

The accuracy of genomic prediction for key traits in sugarcane remains low relative to other crops, even with growing reference sets. This is potentially due to high-level polyploidy, frequent aneuploidy and complex genetic architectures. Here we aim to assess the impact of genotype transformations typically used in polyploid crops on the accuracy of genomic prediction. We focus on the effect of using &ldquo;diploid&rdquo; genotype calls and aneuploidy. We simulated a sugarcane genome with multi-species ancestry and genome duplications, resulting in 96 chromosomes. We included a range of allele dosage to phenotype maps across ploidies including additive linear, dominant, non-linear sigmoid and log shaped maps. Genomic prediction accuracies were compared from six algorithms including genomic BLUP (GBLUP), extended GBLUP, BayesR, Multilayer perceptron, Convolutional neural network and Attention network. Accuracy of prediction for phenotypes controlled by a higher proportion of additive effects were more sensitive to diploid transformation and aneuploidy events. Conversely, accuracy of prediction for phenotypes involving directional dominance were less affected by diploid transformation but were still substantially affected by aneuploidy. For traits controlled by non-linear allele dosage to phenotype maps, the predictive accuracy was significantly affected by diploid transformation, but the influence of aneuploidy was not obvious. Extended GBLUP performed well and gave higher accuracies than other linear based models across most of scenarios, while the DL models were more competitive when the allele action across ploidies became more complex. Overall, our results demonstrate that genotypes that capture allele dosage should be used in genomic prediction for complex polyploids.


---
# A novel long short-term memory-adaptive feedback-correction gain extended Kalman filter for the high-precision state-of-charge estimation of lithium-ion batteries

## 一种新颖的长短期记忆自适应反馈校正增益扩展卡尔曼滤波器，用于锂离子电池的高精度充电状态估计

Link: https://www.researchsquare.com/article/rs-5409650/latest

Assessing the state of charge (SOC) is essential in guaranteeing the precise and effective use of lithium-ion batteries in electric vehicles and smart devices. For these batteries to continue to be dependable, safe to use, and have an appropriate service life in a variety of applications, such as electric vehicles and portable electronics, accurate SOC estimation by the battery management system (BMS) is essential. To examine the effects of training and testing variables on SOC estimate accuracy, this study makes use of transfer learning in a long short-term memory (LSTM) network. It also focuses on applying an adaptive feedback correction-gain extended Kalman filter (AFGEKF) and an EKF, using independently provided operational data and LSTM-estimated SOCs for performance optimization. Through iteration, this method improves denoising and SOC accuracy in a range of working conditions. The comprehensive results show that the ideal mean absolute error, mean squared error, and mean absolute percentage error are 0.4544%, 0.7326%, and 0.9371% for the LSTM model; 0.3069%, 0.4093%, and 0.3577% for the LSTM-EKF model; and 0.14687%, 0.3169%, and 0.2492% for the proposed LSTM-AFGEKF model at 0&amp;deg;C, 25&amp;deg;C, and 45&amp;deg;C using a ternary battery. The study shows that the training and testing hyperparameters of LSTM have a substantial impact on the accuracy of SOC estimations. Furthermore, the proposed LSTM-AFGEKF model&amp;rsquo;s capacity to provide precise SOC estimations serves as a good model with high computational efficiency.


---
# Cognitive performance classification of older patients using machine learning and electronic medical records

## 使用机器学习和电子病历对老年患者的认知表现进行分类

Link: https://www.researchsquare.com/article/rs-5112323/latest

Dementia is anticipated to rise significantly by 2050, posing a huge challenge to the care and health services. Consequently, it is crucial to develop tools that can speed up the process to obtain the right diagnosis. One promising approach is the use of machine learning (ML) algorithms, which show great potential in improving diagnostic accuracy and identifying risk factors for cognitive decline. This study (n = 283) aims to assess various ML techniques in correctly identifying the level of cognitive performance of older patients using data derived from electronic medical records (EMRs). The most influential factors distinguishing healthy controls from those with MCI are history of myocardial infarction, vitamin D3 levels, IADL, age, and sodium levels, while for distinguishing controls from dementia patients, the key factors are IADL, ADL, years of education,vitamin D3 levels, and age. The obtained results indicate that the nonlinear Support vector machine (SVM) is the most effective in discriminating between control subjects and patients with MCI (ACC= 70%; AUC=0.79) and dementia (ACC=87%;AUC=0.94), outperforming other ML models such as Random Forest, K-Nearest Neighbors (KNN) and neural networks. It is expected that these results may significantly improve the diagnosis process.


---
# Hybrid Cloud-Edge AI Framework for Real-Time Predictive Analytics in Digital Twin Healthcare Systems

## 用于数字孪生医疗保健系统中实时预测分析的混合云边缘AI框架

Link: https://www.researchsquare.com/article/rs-5412158/latest

The development of Digital Twin (DT) systems for healthcare introduces new opportunities and challenges, particularly in the areas of real-time processing and predictive analytics. This research presents a hybrid cloud-edge architecture designed to improve the efficiency and responsiveness of remote healthcare monitoring. By integrating edge computing with cloud resources, the system facilitates real-time data transfer and decision-making based on live patient met-rics. A hybrid machine learning model, combining Multi-Layer Perceptron (MLP) and XGBoost, was applied to predict key health indicators, such as heart rate, blood oxygen saturation, body temperature, and diabetes mellitus. Far-edge and near-edge devices were deployed to handle data processing at various stages, reducing latency and improving overall system performance. The results demonstrate significant improvements, with a 40% reduction in latency, a 15% increase in efficiency, and 30% improvement in throughput compared to non-DT systems. The hybrid AI model exhibited robust performance, with cross-validation accuracy reaching 97.48% and real-time accuracy measured at 95.45%. Additionally, it reduced testing time by 58.4%, highlighting its efficiency for real-time healthcare applications. This framework enhances the scalability, efficiency, and reliability of remote healthcare systems, offering a practical solution for real-time patient monitoring in healthcare applications.


---
# ANN-UKF-Based Estimator for Landing Forces in Quadruped Robots

## 基于ANN-UKF的四足机器人着陆力估计器

Link: https://www.researchsquare.com/article/rs-5183842/latest

Estimation of landing forces is essential for legged robots to execute precise and effective movements, particularly when navigating complex and unfamiliar terrains. Traditionally, this estimation is achieved using force sensors mounted on the robot's legs. This paper introduces a novel approach that eliminates the need for force sensors by employing a force-sensorless estimator. The proposed method utilizes an artificial neural network (ANN) to forecast joint torques at the legs at specific points along the motion trajectory. This ANN model functions as a predictor within an Unscented Kalman Filter (UKF), which refines and stabilizes the force measurements. By analyzing the differences between predicted and filtered values, the method estimates both the state and magnitude of the robot&amp;rsquo;s foot impact with the terrain. The effectiveness of this approach has been validated through simulations and practical tests on a quadruped robot.


---
# Prediction of oxidation resistance and mechanism study of Ti-V-Cr burn resistant titanium alloy based on machine learning

## 基于机器学习的ti-v-cr阻燃钛合金抗氧化性能预测及机理研究

Link: https://www.researchsquare.com/article/rs-5400377/latest

A machine learning model was developed to predict the oxidation resistance of Ti-V-Cr burn resistant titanium alloy and the natural logarithm of the parabolic oxidation rate constant (lnkp) was utilized as the model output. Four algorithms were used to train the model. The results show that the two algorithms based on multiple learners, Gradient Boosting Decision Tree (GBDT) and eXtreme Gradient Boosting (XGBoost) show better performance. The coefficient of determination R2 of the model is 0.98 and the maximum error is 6.57% and 6.40% respectively. The importance and interpretability of the input features such as the content and temperature were analyzed. It was found that the trend of the model analysis results was the same as that of the experimental conclusions, which further revealed the mechanism of the influence of element content and temperature changes on the oxidation resistance of Ti-V-Cr alloys and verified the effectiveness of the model. This study is of great significance for the discovery, prediction and quantification of new high temperature oxidation resistant Ti-V-Cr alloys.


---
# Generating Learning Guides for Medical Education with Large Language Models and Statistical Analysis of Test Results

## 用大语言模型生成医学教育学习指南，并对测试一下结果进行统计分析

Link: https://www.researchsquare.com/article/rs-5404292/latest

Background:&amp;nbsp;The Progress Test Medizin (PTM) is a formative test for medical students issued twice a year by the Charit&eacute;-Universit&auml;tsmedizin Berlin. We used data from the PTM to automatically generate personalized feedback designed to identify the knowledge gaps of test participants as exactly as possible with the help of large language models and statistical analysis.&amp;nbsp;
Methods: We have developed a seven-step approach to fulfil the purpose of this study. Firstly, a large language model (ChatGPT 4.0) identified keywords in the form of MeSH terms from all 200 questions of one PTM run. These keywords were checked against the list of medical terms included in the MeSH thesaurus published by the National Library of Medicine (NLM). Meanwhile, answer patterns of PTM questions were also analysed to find whether, given a pair of questions (A,B), knowing the answer to A has a large effect on improving the chance of knowing the answer to B. If that is the case, we say that question A is a precursor question to question B. With this information, we obtained series of questions related to specific MeSH terms and used them to develop a framework that allowed us to assess the performance of PTM participants and compose personalized feedback structured around a curated list of medical topics. A clustering procedure was also applied to construct benchmarking sets.&amp;nbsp;&amp;nbsp;
Results: We simulated the generation of personalized feedback for 1,401 PTM participants out of their test results, thereby producing specific information about their knowledge regarding a number of topics ranging from 34 to 243. Substantial knowledge gaps were found in 14.67% to 21.76% of rated learning topics, depending on the benchmarking set considered.&amp;nbsp;&amp;nbsp;
Conclusion:&amp;nbsp; We designed and tested a method to generate student feedback covering up to 243 medical topics defined by MeSH terms. The feedback received by students in the later stages of their studies was more detailed, as they tend to face more questions matching their knowledge level.


---
# GRBMTI: A Multi-Feature Fusion Approach Combining GraRep and RNA2vec for MiRNA-MRNA Interaction Prediction

## GRBMTI: 一种结合GraRep和RNA2vec的多特征融合方法，用于预测mirna-mrna相互作用

Link: https://www.researchsquare.com/article/rs-5400619/latest

MicroRNA (miRNA) interactions with messenger RNA (mRNA) are critical in biological processes, and predicting these interactions is crucial for understanding their mechanisms. Given the limitations of traditional biological experimental methods, developing appropriate predictive models to generate high-quality potential targets has become increasingly prevalent in this field. However, current predictive methods are problematic because they rely solely on potential miRNA target sites instead of utilizing the entire mRNA sequence, leading to potential feature loss. Considering the limitations of current predictive methods, we introduce a new deep learning model, GRBMTI, designed to fully utilize both the node and sequence features of miRNA and mRNA to enhance the predictive performance of miRNA-targeting mRNA interactions. For feature extraction, we use RNA2vec to train on RNA data and obtain RNA word vector representations, followed by using CNN and BiGRU to mine RNA sequence features, while GraRep is used for obtaining node features. Finally, DNN is employed to merge sequence and node features, enhancing the integration of features to accurately predict miRNA-mRNA interactions. The GRBMTI model demonstrates robust performance on the MTIS-9214 dataset, with an accuracy of 85.89%, AUC of 0.9389, and AUPR of 0.9392, alongside high cross-dataset consistency. These results underscore its notable referential value for advancing the study of miRNA-target mRNA interactions, indicating its utility and relevance in the field.


---
# Simulation comparison of the effects of missing data imputation methods on classification performance in high dimensional data

## 高维数据中缺失数据填补方法对分类性能影响的仿真比较

Link: https://www.researchsquare.com/article/rs-5407580/latest

The study aims to examine the performance of different missing data imputation methods on accurately estimating missing data in high dimensional datasets and their impact on classification using extreme learning machines (ELM). Random datasets were generated with n&amp;thinsp;=&amp;thinsp;150 observations, p&amp;thinsp;=&amp;thinsp;500 independent variables, and different missing data rates. Various imputation methods were used, including mean, median, random, k-nearest neighbors (KNN), missing value imputation with random forests (I-RF), multivariate imputations by chained equations with classification and regression trees (MICE-CART), as well as direct and indirect use of regularized regression (DURR and IURR) methods specifically developed for high dimensional data. The performance of the methods was evaluated based on their proximity to the reference classification scores obtained using ELM. I-RF, MICE-CART, DURR, and IURR, followed by KNN methods, exhibited better performance at low missing rates, while DURR and IURR methods stood out at high missing rates.


---
# Fast simulation of the Zero Degree Calorimeter responses with generative neural networks

## 使用生成神经网络快速模拟零度量热仪响应

Link: https://www.researchsquare.com/article/rs-5145079/latest

Applying machine learning methods to high-energy physics simulations has recently emerged as a rapidly developing area. A prominent example is the Zero Degree Calorimeter (ZDC) simulation in the ALICE experiment at CERN, where substituting the traditional computationally extensive Monte Carlo methods with generative models radically reduces computation time. Although numerous studies have addressed the fast ZDC simulation, there remains significant potential for innovations. Recent developments in generative neural networks have enabled the creation of models capable of producing high-quality samples indistinguishable from real data. In this paper, we apply the latest advances to the simulation of the ZDC neutron detector and achieve a significant improvement in the Wasserstein metric compared to existing methods with a low generation time of 5 ms per sample. Our focus is on exploring novel architectures and state-of-the-art generative frameworks. We compare their performance against established methods, demonstrating competitive outcomes in speed and efficiency. The source code and hyperparameters of the models can be found at https://github.com/m-wojnar/zdc.


---
# Hormones and science learning: New insights into adolescent motivation

## 激素和科学学习: 对青少年动机的新见解

Link: https://www.researchsquare.com/article/rs-5522098/latest

The decline in students' motivation to learn science during adolescence is a widespread phenomenon documented across many countries. While social and educational factors affecting this decline are well-studied, the role of pubertal physiological changes remains unexplored. Here we show that changes in dehydroepiandrosterone sulfate (DHEA-S) and testosterone levels correlate with shifts in students' science learning motivation, particularly during elementary school. Our study of elementary and junior high school students reveals that higher levels of these hormones associate with lower overall motivation in science during elementary years, primarily affecting personal motivation and perceived parental emphasis. This relationship becomes less pronounced in junior high school, suggesting a critical window for hormone-sensitive motivational changes in early adolescence. These findings bridge endocrinology and education research, demonstrating how biological development influences academic motivation during puberty. Our research provides a new framework for understanding the complex interplay between physiological changes and educational outcomes, suggesting the need for developmentally-informed approaches to maintaining students' engagement with science.


---
# Developing the new diagnostic model by integrating bioinformatics and machine learning for osteoarthritis

## 通过整合生物信息学和机器学习开发新的骨关节炎诊断模型

Link: https://www.researchsquare.com/article/rs-5404316/latest

Background: Osteoarthritis (OA) is a common cause of disability among the elderly, profoundly affecting quality of life. This study aims to leverage bioinformatics and machine learning to develop an artificial neural network (ANN) model for diagnosing OA, providing new avenues for early diagnosis and treatment.
Methods:From the Gene Expression Omnibus (GEO) database, we first obtained OA synovial tissue microarray datasets. Differentially expressed genes (DEGs) associated with OA were identified through utilization of the Limma package and weighted gene co-expression network analysis (WGCNA). Subsequently, protein-protein interaction (PPI) network analysis and machine learning were employed to identify the most relevant potential signature genes of OA，and ANN diagnostic model and receiver operating characteristic (ROC) curve were constructed to evaluate the diagnostic performance of the model. Finally, immune cell infiltration analysis was performed using CIBERSORT algorithm to explore the correlation between signature genes and immune cells.
Results: The Limma package and WGCNA identified a total of 72 DEGs related to OA，of which 12 were up-regulated and 60 were down-regulated. Then, the PPI network analysis identified 21 hub genes, and three machine learning algorithms finally screened four feature genes (BTG2, CALML4, DUSP5, and GADD45B). The ANN diagnostic model was constructed based on these four feature genes. The AUC of the training set was 0.942, and the AUC of the validation set was 0.850. Immune cell infiltration analysis revealed B cells memory, T cells gamma delta, B cells naive, Plasma cells, T cells CD4 memory resting, and NK cells The abnormal infiltration of activated cells may be related to the progression of OA.
Conclusions: In this study, BTG2, CALML4, DUSP5, and GADD45B were identified as potential characteristic genes for OA, and an ANN diagnostic model with excellent diagnostic performance has been developed. Therefore, the diagnostic model established in this research can serve as a reliable reference for early OA diagnosis and provide a novel perspective on the pathogenesis of OA.


---
# Securing Automotive Networks from DoS and Fuzzy Attacks with Optimized LSTM Models

## 使用优化的LSTM模型保护汽车网络免受DoS和模糊攻击

Link: https://www.researchsquare.com/article/rs-5157195/latest

The rapid development of intelligently connected automobiles has been greatly facilitated by the profound integration of technologically advanced networked devices and automotive innovation. The Controller Area Network (CAN) communicates to all connected devices called nodes inside a vehicle without verifying the addresses of those nodes to facilitate network connectivity within the vehicle. Because of this, it is highly susceptible since it lacks the authentication and authorization protocols, leading to all possible attack varieties. By integrating the Bacterial Foraging Optimization (BFO) technique with a Long Short-Term Memory (LSTM) neural network, the system presents a novel idea to identify and avert fuzzy and Denial of Service (DoS) attacks on the CAN bus system. The effectiveness of the suggested method is proven by extensive experiments conducted on real-world car hacking datasets. The related CAN data features optimized for feature selection can be retrieved using the BFO algorithm. The LSTM model's features identify irregular CAN bus activity by learning complicated temporal patterns and capturing long-term dependencies. The methods, like rate limits, filtering IDs of high-priority messages, and preventing malicious congestion, are implemented to avoid DoS attacks. System responses to fuzzy attacks include correcting abnormal communication patterns, avoiding random IDs, and filtering attacked data. The results show that the methodology improves the security of automotive networks by detecting CAN bus attacks with high accuracy, low False Positive Rates (FPR), and efficient attack prevention capabilities.


---
# Pediatric Wrist Fracture Detection Using Feature Context Excitation Modules in X-ray Images

## 使用x射线图像中的特征上下文激励模块进行小儿腕部骨折检测

Link: https://www.researchsquare.com/article/rs-5411127/latest

Children often suffer wrist trauma in daily life, while they usually need radiologists to analyze and interpret X-ray images before surgical treatment by surgeons.The development of deep learning has enabled neural networks to serve as computer-assisted diagnosis (CAD) tools to help doctors and experts in medical image diagnostics.Since YOLOv8 model has obtained the satisfactory success in object detection tasks, it has been applied to various fracture detection.This work introduces four variants of Feature Contexts Excitation-YOLOv8 (FCE-YOLOv8) model, each incorporating a different FCE module (i.e., modules of Squeeze-and-Excitation (SE), Global Context (GC), Gather-Excite (GE), and Gaussian Context Transformer (GCT)) to enhance the model performance.Experimental results on GRAZPEDWRI-DX dataset demonstrate that our proposed YOLOv8+GC-M3 model improves the mAP@50 value from 65.78% to 66.32%, outperforming the state-of-the-art (SOTA) model while reducing inference time.Furthermore, our proposed YOLOv8+SE-M3 model achieves the highest mAP@50 value of 67.07%, exceeding the SOTA performance.The implementation of this work is available at https://github.com/RuiyangJu/FCE-YOLOv8.


---
# Exploring dominant strategies in iterated and evolutionary games: a multi-agent reinforcement learning approach

## 探索迭代和进化游戏中的主导策略: 一种多智能体强化学习方法

Link: https://www.researchsquare.com/article/rs-5462150/latest

Exploring dominant strategies in iterated games holds profound theoretical and practical significance across diverse domains. Previous studies, through mathematical analysis of limited cases, have unveiled classic strategies such as tit-for-tat, generous-tit-for-tat, win-stay-lose-shift, and zero-determinant strategies. While these strategies offer valuable insights into human decision-making, they represent only a small subset of possible strategies, constrained by limited mathematical and computational tools available to explore larger strategy spaces.
To bridge this gap, we propose a novel approach using multi-agent reinforcement learning to delve into complex decision-making processes that go beyond human intuition. Our approach has led to the discovery of a new strategy, i.e., memory-two bilateral reciprocity (MTBR) strategy. 
MTBR consistently outperforms a wide range of strategies in pairwise interactions while achieving high payoffs. When introduced into an evolving population with diverse strategies, MTBR demonstrates dominance and fosters higher levels of cooperation and social welfare in both homogeneous and heterogeneous structures, as well as across various game types. This high performance is verified by simulations and mathematical analysis. 
Our work highlights the potential of multi-agent reinforcement learning in uncovering dominant strategies in complex environments, offering a new perspective on strategy exploration in iterated games.


---
# Rediscovering the climate analogue method: insights from a data-driven approach

## 重新发现气候模拟方法: 来自数据驱动方法的见解

Link: https://www.researchsquare.com/article/rs-5382842/latest

Analogue methods are widely used in atmospheric science for weather forecasting and climate risk studies. The concept of weather/climate analogues is straightforward: it quantifies the similarity between weather conditions at target and candidate time periods by computing a distance using relevant atmospheric variables (e.g., temperatures and geopotential heights). Traditionally, Euclidean distance (ED) is used for this task, computed over the original value space of the variables. In recent years, with the rapid advance in deep learning (DL) techniques, the concept of &rsquo;features&rsquo; (or representations) has emerged, providing an alternative for analogue-related applications. This study aims to explore the potential of DL in climate- scale applications. Specifically, we propose a data-driven climate analogue approach, which involves constructing a DL-based climate dynamics model, ClimaDist. This model identifies features relevant to climate dynamics, and we use these features, instead of the original input atmospheric variables, to search for analogues. In addition to the traditional ED, we explored two other distance measures that were not widely used in the atmospheric community: the image Euclidean distance (IMED) and the structural similarity index metric (SSIM). A range of different combinations of models, distance measures and input data settings were tested using ERA5 reanalysis data, and our results suggest that the proposed ClimaDist outperforms all other model combinations in terms of the ability to reproduce both the observed distributions and variations. Finally, we employed explainable AI (XAI) techniques to analyse the effectiveness of ClimaDist in identifying climate analogues. The results indicate that the importance of climate features varies both spatially and temporally, a factor that should be accounted for in analogue identification but is often overlooked in traditional methods.


---
# Machine learning corrects climate model biases in global flood projections

## 机器学习纠正了全球洪水预测中的气候模型偏差

Link: https://www.researchsquare.com/article/rs-5493546/latest

Reliable river flood projections are crucial for the development of effective adaptation and mitigation strategies, but the conventional hydroclimatic modelling chain introduces a &lsquo;cascade of uncertainty&rsquo; arising from climate model outputs and hydrologic model structures. Here, we assess the potential for machine learning (ML) models to overcome this cascade of uncertainty. We develop data-driven projections of 10-year flood magnitudes by training global ML models directly on the uncorrected historical simulation statistics of 19 CMIP6 global climate models alongside geophysical river basin attributes. We find that ML models driven with uncorrected climate model outputs outperform physics-based hydrologic models driven with bias-corrected climate model outputs, with average error rates reduced by 40%. The ML models learn to correct spatially variable biases in the climate model outputs by adjusting the contribution of basin attributes based on the magnitude of each attribute. For example, they compensate for the underestimation of extreme precipitation intensity in the wettest river basins by increasing the contribution of the aridity predictor to flood magnitude. Our global ML projections in 1.66 million river segments (4,697,400 km) suggest that by 2100, over 34% of rivers may experience larger floods, with comparatively little change in the largest rivers in tropical and dry regions.


---
# ST-CFI: Swin Transformer with Convolutional Feature Interactions for Identifying Plant Diseases

## St-cfi: 具有卷积特征交互作用的Swin变换器，用于识别植物病害

Link: https://www.researchsquare.com/article/rs-5350597/latest

Background: The increasing global population, coupled with the diminishing availability of arable land, has rendered the challenge of ensuring food security more pronounced. The prompt and precise identification of plant diseases is essential for reducing crop losses and improving agricultural yield. In this paper, we introduce the Swin Transformer with Convolutional Feature Interactions (STCFI) model, which represents a state-of-the-art deep learning methodology aimed at detecting plant diseases through the analysis of leaf images. The ST-CFI model effectively integrates the strengths of Convolutional Neural Networks (CNNs) and Swin Transformers, enabling the extraction of both local and global features from plant images. This is achieved through the implementation of an inception architecture and cross-channel feature learning, which collectively enhance the information necessary for detailed feature extraction.&amp;nbsp;
Results: We conducted a series of comprehensive experiments utilizing five distinct datasets: PlantVillage, the Plant Pathology 2021 competition, Plant- Doc, AI2018, and iBean. The ST-CFI model exhibited exceptional performance, achieving an accuracy of 99.94% on the PlantVillage dataset, 99.22% on iBean, 86.89% on AI2018, and 77.54% on PlantDoc. These results underscore the model&rsquo;s robustness and its capacity to generalize across various datasets and real-world conditions. The high accuracy and F1 scores, in conjunction with low loss values, further validate the model&rsquo;s efficacy in learning discriminative features.&amp;nbsp;
Conclusion: The ST-CFI model signifies a substantial advancement in the early and accurate detection of plant diseases, serving as a valuable instrument for precision agriculture. Its capacity to integrate CNNs and Transformers within a unified framework enhances the model&rsquo;s feature extraction capabilities, resulting in improved accuracy in the identification of plant diseases. This study concludes that the ST-CFI model is an effective tool for addressing the challenges associated with plant disease detection, with significant implications for agricultural sustainability and productivity.


---
# Using agro-hydrological machine-learning to spatially target investments in sustainable groundwater irrigation

## 使用农业水文机器学习对可持续地下水灌溉进行空间目标投资

Link: https://www.researchsquare.com/article/rs-5424317/latest

Groundwater irrigation supports over 40% of global crop production and stabilizes yields amidst climatic change. Yet, over-abstraction can cause water scarcity, disrupt ecosystems, and increase greenhouse gas emissions. Governments and international financial institutions have made significant investments in sustainable groundwater irrigation but require enhanced spatial targeting to increase impact. In response, this study employs an agro-hydrological machine-learning approach to analyze spatial patterns of (i) crop yield responses to increased irrigation and (ii) groundwater sustainability in South Asia &ndash; characterized by smallholder farming, increasing groundwater dependence, and post-green revolution sustainability challenges. We show that modestly increasing irrigation intensity in groundwater-rich areas with high yield responses could boost rice production by 2.22Mt annually &ndash; sufficient to feed over 33 million people with little anticipated risks of groundwater depletion. However, current investments overlook these areas. Our approach can be globally applied to catalyze sustainable irrigation through integrated use of expanding agricultural and hydrological datasets.

