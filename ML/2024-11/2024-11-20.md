# Learning high-accuracy error decoding for quantum processors

## 学习量子处理器的高精度错误解码

Link: https://www.nature.com/articles/s41586-024-08148-8

<p>Nature, Published online: 20 November 2024; <a href="https://www.nature.com/articles/s41586-024-08148-8">doi:10.1038/s41586-024-08148-8</a></p>A recurrent, transformer-based neural network, called AlphaQubit, learns high-accuracy error decoding to suppress the errors that occur in quantum systems, opening the prospect of using neural-network decoders for real quantum hardware.


---
# AI’s computing gap: academics lack access to powerful chips needed for research

## 人工智能的计算差距: 学者无法获得研究所需的强大芯片

Link: https://www.nature.com/articles/d41586-024-03792-6

<p>Nature, Published online: 20 November 2024; <a href="https://www.nature.com/articles/d41586-024-03792-6">doi:10.1038/d41586-024-03792-6</a></p>Survey highlights disparity between academic and industry scientists’ access to computing power needed to train machine-learning models.


---
# Quantum computing: physics–AI collaboration quashes quantum errors

## 量子计算: 物理学-人工智能合作消除了量子错误

Link: https://www.nature.com/articles/d41586-024-03557-1

<p>Nature, Published online: 20 November 2024; <a href="https://www.nature.com/articles/d41586-024-03557-1">doi:10.1038/d41586-024-03557-1</a></p>A neural network has learnt to correct the errors that arise during quantum computation, outperforming algorithms that were designed by humans. The strategy sets out a promising path towards practical quantum computers.


---
# Calibrating Bayesian generative machine learning for Bayesiamplification

## 校准贝叶斯生成式机器学习以实现贝叶斯放大

Link: http://iopscience.iop.org/article/10.1088/2632-2153/ad9136

Recently, combinations of generative and Bayesian deep learning have been introduced in particle physics for both fast detector simulation and inference tasks. These neural networks aim to quantify the uncertainty on the generated distribution originating from limited training statistics. The interpretation of a distribution-wide uncertainty however remains ill-defined. We show a clear scheme for quantifying the calibration of Bayesian generative machine learning models. For a Continuous Normalizing Flow applied to a low-dimensional toy example, we evaluate the calibration of Bayesian uncertainties from either a mean-field Gaussian weight posterior, or Monte Carlo sampling network weights, to gauge their behaviour on unsteady distribution edges. Well calibrated uncertainties can then be used to roughly estimate the number of uncorrelated truth samples that are equivalent to the generated sample and clearly indicate data amplification for smooth features of the distribution.


---
# SynCoTrain: A Dual Classifier PU-learning Framework for Synthesizability Prediction

## SynCoTrain: 一种用于综合能力预测的双分类器PU学习框架

Link: https://arxiv.org/abs/2411.12011

arXiv:2411.12011v1 Announce Type: new 
Abstract: Material discovery is a cornerstone of modern science, driving advancements in diverse disciplines from biomedical technology to climate solutions. Predicting synthesizability, a critical factor in realizing novel materials, remains a complex challenge due to the limitations of traditional heuristics and thermodynamic proxies. While stability metrics such as formation energy offer partial insights, they fail to account for kinetic factors and technological constraints that influence synthesis outcomes. These challenges are further compounded by the scarcity of negative data, as failed synthesis attempts are often unpublished or context-specific.
  We present SynCoTrain, a semi-supervised machine learning model designed to predict the synthesizability of materials. SynCoTrain employs a co-training framework leveraging two complementary graph convolutional neural networks: SchNet and ALIGNN. By iteratively exchanging predictions between classifiers, SynCoTrain mitigates model bias and enhances generalizability. Our approach uses Positive and Unlabeled (PU) Learning to address the absence of explicit negative data, iteratively refining predictions through collaborative learning.
  The model demonstrates robust performance, achieving high recall on internal and leave-out test sets. By focusing on oxide crystals, a well-characterized material family with extensive experimental data, we establish SynCoTrain as a reliable tool for predicting synthesizability while balancing dataset variability and computational efficiency. This work highlights the potential of co-training to advance high-throughput materials discovery and generative research, offering a scalable solution to the challenge of synthesizability prediction.


---
# Cartesian Atomic Moment Machine Learning Interatomic Potentials

## 笛卡尔原子矩机器学习原子间势

Link: https://arxiv.org/abs/2411.12096

arXiv:2411.12096v1 Announce Type: new 
Abstract: Machine learning interatomic potentials (MLIPs) have substantially advanced atomistic simulations in materials science and chemistry by providing a compelling balance between accuracy and computational efficiency. While leading MLIPs rely on representations of atomic environments using spherical tensors, Cartesian representations offer potential advantages in simplicity and efficiency. In this work, we introduce Cartesian Atomic Moment Potentials (CAMP), an approach equivalent to models based on spherical tensors but operating entirely in the Cartesian space. CAMP constructs atomic moment tensors from neighboring atoms and combines these through tensor products to incorporate higher body-order interactions, which can provide a complete description of local atomic environments. By integrating these into a graph neural network (GNN) framework, CAMP enables physically-motivated and systematically improvable potentials. It requires minimal hyperparameter tuning that simplifies the training process. The model demonstrates excellent performance across diverse systems, including periodic structures, small organic molecules, and two-dimensional materials. It achieves accuracy, efficiency, and stability in molecular dynamics simulations surpassing or comparable to current leading models. By combining the strengths of Cartesian representations with the expressiveness of GNNs, CAMP provides a powerful tool for atomistic simulations to accelerate materials understanding and discovery.


---
# Hidden Markov model analysis to fluorescence blinking of fluorescently labeled DNA

## 隐马尔可夫模型分析荧光标记DNA的荧光闪烁

Link: https://arxiv.org/abs/2411.12176

arXiv:2411.12176v1 Announce Type: new 
Abstract: We examine quantitatively the transition process from emitting to not-emitting states of fluorescent molecules with a machine learning technique. In a fluorescently labeled DNA, the fluorescence occurs continuously under irradiation, but it often transfers to the not-emitting state corresponding to a charge-separated state. The trajectory of the fluorescence consists of repetitions of light-emitting (ON) and not-emitting (OFF) states, called blinking, and it contains a very large amount of noise due to the several reasons, so in principle, it is difficult to distinguish the ON and OFF states quantitatively. The fluorescence trajectory is a typical stochastic process, and therefore requires advanced time-series data analysis. In the present study, we analyze the fluorescence trajectories using a hidden Markov model, and calculate the probability density of the ON and OFF duration. From the analysis, we found that the ON-duration probability density can be well described by an exponential function, and the OFF-duration probability density can be well described by a log-normal function, which are verified in terms of Kolmogorov-Smirnov test. The time-bin dependence in the fluorescence trajectory on the probability density is carefully analyzed. We also discuss the ON and OFF processes from failure-rate analysis used in life testing of semiconductor devices.


---
# Neural Network-Based Tensor Model for Nematic Liquid Crystals with Accurate Microscopic Information

## 基于神经网络的精确微观信息向列相液晶张量模型

Link: https://arxiv.org/abs/2411.12224

arXiv:2411.12224v1 Announce Type: new 
Abstract: The phenomenological Landau-de Gennes (LdG) model is a powerful continuum theory to describe the macroscopic state of nematic liquid crystals. However, it is invariably less accurate and less physically informed than the molecular-level models due to the lack of physical meaning of the parameters. We propose a neural network-based tensor (NN-Tensor) model for nematic liquid crystals, supervised by the molecular model. Consequently, the NN-Tensor model not only attains energy precision comparable to the molecular model but also accurately captures the Isotropic-Nematic phase transition, which the LdG model cannot achieve. The NN-Tensor model is further embedded in another neural network to predict liquid crystal configurations in a domain-free and mesh-free manner. We apply the NN-Tensor model to nematic liquid crystals in a number of two-dimensional and three-dimensional domains to demonstrate it can efficiently identify rich liquid crystal configurations in both regular and non-regular confinements.


---
# MAViS: Modular Autonomous Virtualization System for Two-Dimensional Semiconductor Quantum Dot Arrays

## MAViS: 二维半导体量子点阵列的模块化自主虚拟化系统

Link: https://arxiv.org/abs/2411.12516

arXiv:2411.12516v1 Announce Type: new 
Abstract: Arrays of gate-defined semiconductor quantum dots are among the leading candidates for building scalable quantum processors. High-fidelity initialization, control, and readout of spin qubit registers require exquisite and targeted control over key Hamiltonian parameters that define the electrostatic environment. However, due to the tight gate pitch, capacitive crosstalk between gates hinders independent tuning of chemical potentials and interdot couplings. While virtual gates offer a practical solution, determining all the required cross-capacitance matrices accurately and efficiently in large quantum dot registers is an open challenge. Here, we establish a Modular Automated Virtualization System (MAViS) -- a general and modular framework for autonomously constructing a complete stack of multi-layer virtual gates in real time. Our method employs machine learning techniques to rapidly extract features from two-dimensional charge stability diagrams. We then utilize computer vision and regression models to self-consistently determine all relative capacitive couplings necessary for virtualizing plunger and barrier gates in both low- and high-tunnel-coupling regimes. Using MAViS, we successfully demonstrate accurate virtualization of a dense two-dimensional array comprising ten quantum dots defined in a high-quality Ge/SiGe heterostructure. Our work offers an elegant and practical solution for the efficient control of large-scale semiconductor quantum dot systems.


---
# Learning complexity gradually in quantum machine learning models

## 在量子机器学习模型中逐渐学习复杂性

Link: https://arxiv.org/abs/2411.11954

arXiv:2411.11954v1 Announce Type: cross 
Abstract: Quantum machine learning is an emergent field that continues to draw significant interest for its potential to offer improvements over classical algorithms in certain areas. However, training quantum models remains a challenging task, largely because of the difficulty in establishing an effective inductive bias when solving high-dimensional problems. In this work, we propose a training framework that prioritizes informative data points over the entire training set. This approach draws inspiration from classical techniques such as curriculum learning and hard example mining to introduce an additional inductive bias through the training data itself. By selectively focusing on informative samples, we aim to steer the optimization process toward more favorable regions of the parameter space. This data-centric approach complements existing strategies such as warm-start initialization methods, providing an additional pathway to address performance challenges in quantum machine learning. We provide theoretical insights into the benefits of prioritizing informative data for quantum models, and we validate our methodology with numerical experiments on selected recognition tasks of quantum phases of matter. Our findings indicate that this strategy could be a valuable approach for improving the performance of quantum machine learning models.


---
# Construction of Multi-Dimensional Functions for Optimization of Additive-Manufacturing Process Parameters

## 增材制造工艺参数优化的多维函数构建

Link: https://arxiv.org/abs/2311.06398

arXiv:2311.06398v3 Announce Type: replace 
Abstract: The authors present a generic framework for the parameter optimization of additive manufacturing (AM) processes, one tailored to a high-throughput experimental methodology (HTEM). Given the large number of parameters, which impact the quality of AM-metallic components, the authors advocate for partitioning the AM parameter set into stages (tiers), based on their relative importance, modeling one tier at a time until successful, and then systematically expanding the framework. The authors demonstrate how the construction of multi-dimensional functions, based on neural networks (NN), can be applied to successfully model relative densities and Rockwell hardness obtained from HTEM testing of the Inconel 718 superalloy fabricated, using a powder-bed approach. The authors analyze the input data set, assess its suitability for predictions, and show how to optimize the framework for the multi-dimensional functional construction, such as to obtain the highest degree of fit with the input data set. The authors also compare and contrast the NN-based multi-dimensional functional construction to multi-variate linear regression, to polynomial regression, and to Gaussian process regression (GPR), highlight similarity between the NN-based multi-dimensional functional construction and the GPR, and offer insights into the suitability of each of these methods for the data set and the application at hand. In terms of the coefficient of determination, $R^2$, a relatively simple, single-layer NN with 5 or 10 nodes outperforms multi-variate linear regression, 2nd-order polynomial regression, and GPR for the primary Inconel 718 HTEM data set studied. The novelty of the research work entails the versatile and scalable NN framework presented, suitable for use in conjunction with HTEM, for the AM parameter optimization of superalloys, and beyond.


---
# A universal approximation theorem for nonlinear resistive networks

## 非线性电阻网络的一个通用逼近定理

Link: https://arxiv.org/abs/2312.15063

arXiv:2312.15063v2 Announce Type: replace-cross 
Abstract: Resistor networks have recently attracted interest as analog computing platforms for machine learning, particularly due to their compatibility with the Equilibrium Propagation training framework. In this work, we explore the computational capabilities of these networks. We prove that electrical networks consisting of voltage sources, linear resistors, diodes, and voltage-controlled voltage sources (VCVS) can approximate any continuous function to arbitrary precision. Central to our proof is a method for translating a ReLU neural network into an approximately equivalent electrical network comprising these four elements. Our proof relies on two assumptions: (a) circuit elements are ideal, and (b) variable resistor conductances and VCVS amplification factors can take any value (arbitrarily small or large). Our findings provide insights that could guide the development of universal self-learning electrical networks.


---
# Variable Rate Neural Compression for Sparse Detector Data

## 稀疏检测器数据的可变速率神经压缩

Link: https://arxiv.org/abs/2411.11942

arXiv:2411.11942v1 Announce Type: new 
Abstract: High-energy large-scale particle colliders generate data at extraordinary rates. Developing real-time high-throughput data compression algorithms to reduce data volume and meet the bandwidth requirement for storage has become increasingly critical. Deep learning is a promising technology that can address this challenging topic. At the newly constructed sPHENIX experiment at the Relativistic Heavy Ion Collider, a Time Projection Chamber (TPC) serves as the main tracking detector, which records three-dimensional particle trajectories in a volume of a gas-filled cylinder. In terms of occupancy, the resulting data flow can be very sparse reaching $10^{-3}$ for proton-proton collisions. Such sparsity presents a challenge to conventional learning-free lossy compression algorithms, such as SZ, ZFP, and MGARD. In contrast, emerging deep learning-based models, particularly those utilizing convolutional neural networks for compression, have outperformed these conventional methods in terms of compression ratios and reconstruction accuracy. However, research on the efficacy of these deep learning models in handling sparse datasets, like those produced in particle colliders, remains limited. Furthermore, most deep learning models do not adapt their processing speeds to data sparsity, which affects efficiency. To address this issue, we propose a novel approach for TPC data compression via key-point identification facilitated by sparse convolution. Our proposed algorithm, BCAE-VS, achieves a $75\%$ improvement in reconstruction accuracy with a $10\%$ increase in compression ratio over the previous state-of-the-art model. Additionally, BCAE-VS manages to achieve these results with a model size over two orders of magnitude smaller. Lastly, we have experimentally verified that as sparsity increases, so does the model's throughput.


---
# Perfecting Imperfect Physical Neural Networks with Transferable Robustness using Sharpness-Aware Training

## 使用锐度感知训练完善具有可转移鲁棒性的不完善物理神经网络

Link: https://arxiv.org/abs/2411.12352

arXiv:2411.12352v1 Announce Type: new 
Abstract: AI models are essential in science and engineering, but recent advances are pushing the limits of traditional digital hardware. To address these limitations, physical neural networks (PNNs), which use physical substrates for computation, have gained increasing attention. However, developing effective training methods for PNNs remains a significant challenge. Current approaches, regardless of offline and online training, suffer from significant accuracy loss. Offline training is hindered by imprecise modeling, while online training yields device-specific models that can't be transferred to other devices due to manufacturing variances. Both methods face challenges from perturbations after deployment, such as thermal drift or alignment errors, which make trained models invalid and require retraining. Here, we address the challenges with both offline and online training through a novel technique called Sharpness-Aware Training (SAT), where we innovatively leverage the geometry of the loss landscape to tackle the problems in training physical systems. SAT enables accurate training using efficient backpropagation algorithms, even with imprecise models. PNNs trained by SAT offline even outperform those trained online, despite modeling and fabrication errors. SAT also overcomes online training limitations by enabling reliable transfer of models between devices. Finally, SAT is highly resilient to perturbations after deployment, allowing PNNs to continuously operate accurately under perturbations without retraining. We demonstrate SAT across three types of PNNs, showing it is universally applicable, regardless of whether the models are explicitly known. This work offers a transformative, efficient approach to training PNNs, addressing critical challenges in analog computing and enabling real-world deployment.


---
# Leadsee-Precip: A Deep Learning Diagnostic Model for Precipitation

## Leadsee-precip: 降水的深度学习诊断模型

Link: https://arxiv.org/abs/2411.12640

arXiv:2411.12640v1 Announce Type: new 
Abstract: Recently, deep-learning weather forecasting models have surpassed traditional numerical models in terms of the accuracy of meteorological variables. However, there is considerable potential for improvements in precipitation forecasts, especially for heavy precipitation events. To address this deficiency, we propose Leadsee-Precip, a global deep learning model to generate precipitation from meteorological circulation fields. The model utilizes an information balance scheme to tackle the challenges of predicting heavy precipitation caused by the long-tail distribution of precipitation data. Additionally, more accurate satellite and radar-based precipitation retrievals are used as training targets. Compared to artificial intelligence global weather models, the heavy precipitation from Leadsee-Precip is more consistent with observations and shows competitive performance against global numerical weather prediction models. Leadsee-Precip can be integrated with any global circulation model to generate precipitation forecasts. But the deviations between the predicted and the ground-truth circulation fields may lead to a weakened precipitation forecast, which could potentially be mitigated by further fine-tuning based on the predicted circulation fields.


---
# Machine Learning Evaluation Metric Discrepancies across Programming Languages and Their Components: Need for Standardization

## 跨编程语言及其组件的机器学习评估度量差异: 需要标准化

Link: https://arxiv.org/abs/2411.12032

arXiv:2411.12032v1 Announce Type: cross 
Abstract: This study evaluates metrics for tasks such as classification, regression, clustering, correlation analysis, statistical tests, segmentation, and image-to-image (I2I) translation. Metrics were compared across Python libraries, R packages, and Matlab functions to assess their consistency and highlight discrepancies. The findings underscore the need for a unified roadmap to standardize metrics, ensuring reliable and reproducible ML evaluations across platforms. This study examined a wide range of evaluation metrics across various tasks and found only some to be consistent across platforms, such as (i) Accuracy, Balanced Accuracy, Cohens Kappa, F-beta Score, MCC, Geometric Mean, AUC, and Log Loss in binary classification; (ii) Accuracy, Cohens Kappa, and F-beta Score in multi-class classification; (iii) MAE, MSE, RMSE, MAPE, Explained Variance, Median AE, MSLE, and Huber in regression; (iv) Davies-Bouldin Index and Calinski-Harabasz Index in clustering; (v) Pearson, Spearman, Kendall's Tau, Mutual Information, Distance Correlation, Percbend, Shepherd, and Partial Correlation in correlation analysis; (vi) Paired t-test, Chi-Square Test, ANOVA, Kruskal-Wallis Test, Shapiro-Wilk Test, Welchs t-test, and Bartlett's test in statistical tests; (vii) Accuracy, Precision, and Recall in 2D segmentation; (viii) Accuracy in 3D segmentation; (ix) MAE, MSE, RMSE, and R-Squared in 2D-I2I translation; and (x) MAE, MSE, and RMSE in 3D-I2I translation. Given observation of discrepancies in a number of metrics (e.g. precision, recall and F1 score in binary classification, WCSS in clustering, multiple statistical tests, and IoU in segmentation, amongst multiple metrics), this study concludes that ML evaluation metrics require standardization and recommends that future research use consistent metrics for different tasks to effectively compare ML techniques and solutions.


---
# Variational learning of integrated quantum photonic circuits

## 集成量子光子电路的变分学习

Link: https://arxiv.org/abs/2411.12417

arXiv:2411.12417v1 Announce Type: cross 
Abstract: Integrated photonic circuits play a crucial role in implementing quantum information processing in the noisy intermediate-scale quantum (NISQ) era. Variational learning is a promising avenue that leverages classical optimization techniques to enhance quantum advantages on NISQ devices. However, most variational algorithms are circuit-model-based and encounter challenges when implemented on integrated photonic circuits, because they involve explicit decomposition of large quantum circuits into sequences of basic entangled gates, leading to an exponential decay of success probability due to the non-deterministic nature of photonic entangling gates. Here, we present a variational learning approach for designing quantum photonic circuits, which directly incorporates post-selection and elementary photonic elements into the training process. The complicated circuit is treated as a single nonlinear logical operator, and a unified design is discovered for it through variational learning. Engineering an integrated photonic chip with automated control, we adjust and optimize the internal parameters of the chip in real time for task-specific cost functions. We utilize a simple case of designing photonic circuits for a single ancilla CNOT gate with improved success rate to illustrate how our proposed approach works, and then apply the approach in the first demonstration of quantum stochastic simulation using integrated photonics.


---
# Universal programmable waveguide arrays

## 通用可编程波导阵列

Link: https://arxiv.org/abs/2411.12610

arXiv:2411.12610v1 Announce Type: cross 
Abstract: Implementing arbitrary unitary transformations is crucial for applications in quantum computing, signal processing, and machine learning. Unitaries govern quantum state evolution, enabling reversible transformations critical in quantum tasks like cryptography and simulation and playing key roles in classical domains such as dimensionality reduction and signal compression. Integrated optical waveguide arrays have emerged as a promising platform for these transformations, offering scalability for both quantum and classical systems. However, scalable and efficient methods for implementing arbitrary unitaries remain challenging. Here, we present a theoretical framework for realizing arbitrary unitary matrices through programmable waveguide arrays (PWAs). We provide a mathematical proof demonstrating that cascaded PWAs can implement any unitary matrix within practical constraints, along with a numerical optimization method for customized PWA designs. Our results establish PWAs as a universal and scalable architecture for quantum photonic computing, effectively bridging quantum and classical applications, and positioning PWAs as an enabling technology for advancements in quantum simulation, machine learning, secure communication, and signal processing.


---
# NDUI+: A fused DMSP-VIIRS based global normalized difference urban index dataset

## NDUI: 基于DMSP-VIIRS的全球归一化差异城市指数数据集

Link: https://arxiv.org/abs/2306.02794

arXiv:2306.02794v2 Announce Type: replace 
Abstract: Urbanization is advancing rapidly, covering less than 2% of Earth's surface yet profoundly influencing global environments and experiencing disproportionate impacts from extreme weather events. Effective urban management and planning require high-resolution, temporally consistent datasets that capture the complexity of urban growth and dynamics. This study presents NDUI+, a novel global urban dataset addressing critical gaps in urban data continuity and quality. NDUI+ integrates data from the Defense Meteorological Satellite Program's Operational Linescan System (DMSP-OLS), VIIRS Nighttime Light, and Landsat 7 NDVI using advanced remote sensing and deep learning techniques. The dataset resolves sensor discontinuity challenges, offering a seamless 30-meter spatial and annual temporal resolution time series from 1999 to the present. NDUI+ demonstrates high precision and granularity, aligning closely with high-resolution satellite data and capturing urban dynamics effectively. The dataset provides valuable insights for urban climate studies, IPCC assessments, and urbanization research, complementing resources like UT-GLOBUS for urban modeling.


---
# Scientific Machine Learning Based Reduced-Order Models for Plasma Turbulence Simulations

## 基于科学机器学习的等离子体湍流模拟降阶模型

Link: https://arxiv.org/abs/2401.05972

arXiv:2401.05972v3 Announce Type: replace 
Abstract: This paper investigates non-intrusive Scientific Machine Learning (SciML) Reduced-Order Models (ROMs) for plasma turbulence simulations. In particular, we focus on Operator Inference (OpInf) to build low-cost physics-based ROMs from data for such simulations. As a representative example, we consider the (classical) Hasegawa-Wakatani (HW) equations used for modeling two-dimensional electrostatic drift-wave turbulence. For a comprehensive perspective of the potential of OpInf to construct predictive ROMs, we consider three setups for the HW equations by varying a key parameter, namely the adiabaticity coefficient. These setups lead to the formation of complex and nonlinear dynamics, which makes the construction of predictive ROMs of any kind challenging. We generate the training datasets by performing direct numerical simulations of the HW equations and recording the computed state data and outputs the over a time horizon of $100$ time units in the turbulent phase. We then use these datasets to construct OpInf ROMs for predictions over $400$ additional time units, that is, $400\%$ more than the training horizon. Our results show that the OpInf ROMs capture important statistical features of the turbulent dynamics and generalize beyond the training time horizon while reducing the computational effort of the high-fidelity simulation by up to five orders of magnitude. In the broader context of fusion research, this shows that non-intrusive SciML ROMs have the potential to drastically accelerate numerical studies, which can ultimately enable tasks such as the design of optimized fusion devices.


---
# Graph Neural Network-Based Track Finding in the LHCb Vertex Detector

## LHCb顶点检测器中基于图神经网络的航迹查找

Link: https://arxiv.org/abs/2407.12119

arXiv:2407.12119v4 Announce Type: replace 
Abstract: The next decade will see an order of magnitude increase in data collected by high-energy physics experiments, driven by the High-Luminosity LHC (HL-LHC). The reconstruction of charged particle trajectories (tracks) has always been a critical part of offline data processing pipelines. The complexity of HL-LHC data will however increasingly mandate track finding in all stages of an experiment's real-time processing. This paper presents a GNN-based track-finding pipeline tailored for the Run 3 LHCb experiment's vertex detector and benchmarks its physics performance and computational cost against existing classical algorithms on GPU architectures. A novelty of our work compared to existing GNN tracking pipelines is batched execution, in which the GPU evaluates the pipeline on hundreds of events in parallel. We evaluate the impact of neural-network quantisation on physics and computational performance, and comment on the outlook for GNN tracking algorithms for other parts of the LHCb track-finding pipeline.


---
# PZT Optical Memristors

## PZT光学忆阻器

Link: https://arxiv.org/abs/2411.04665

arXiv:2411.04665v3 Announce Type: replace 
Abstract: Optical memristors represent a monumental leap in the fusion of photonics and electronics, heralding a new era of new applications from neuromorphic computing to artificial intelligence. However, current technologies are hindered by complex fabrication, limited endurance, high optical loss or low modulation efficiency. For the first time, we unprecedentedly reveal optical non-volatility in thin-film Lead Zirconate Titanate (PZT) by electrically manipulating the ferroelectric domains to control the refractive index, providing a brand-new routine for optical memristors. The developed PZT optical memristors offer unprecedented advantages more than exceptional performance metrics like low loss, high precision, high-efficiency modulation, high stability quasi-continuity and reconfigurability. The wafer-scale sol-gel fabrication process also ensures compatible with standardized mass fabrication processes and high scalability for photonic integration. Specially, these devices also demonstrate unique functional duality: setting above a threshold voltage enables non-volatile behaviors, below this threshold allows volatile high-speed optical switching. This marks the first-ever optical memristor capable of performing high-speed signal processing and non-volatile retention on a single platform, and is also the inaugural demonstration of scalable functional systems. The PZT optical memristors developed here facilitate the realization of novel paradigms for high-speed and energy-efficient optical interconnects, programmable PICs, quantum computing, neural networks, in-memory computing and brain-like architecture.


---
# Designing a Dataset for Convolutional Neural Networks to Predict Space Groups Consistent with Extinction Laws

## 为卷积神经网络设计数据集以预测符合灭绝定律的空间群

Link: https://arxiv.org/abs/2411.00803

arXiv:2411.00803v2 Announce Type: replace-cross 
Abstract: In this paper, a dataset of one-dimensional powder diffraction patterns was designed with new strategy to train Convolutional Neural Networks for predicting space groups. The diffraction pattern was calculated based on lattice parameters and Extinction Laws, instead of the traditional approach of generating it from a crystallographic database. This paper demonstrates that the new strategy is more effective than the conventional method. As a result, the model trained on the cubic and tetragonal training set from the newly designed dataset achieves prediction accuracy that matches the theoretical maximums calculated based on Extinction Laws. These results demonstrate that machine learning-based prediction can be both physically reasonable and reliable. Additionally, the model trained on our newly designed dataset shows excellent generalization capability, much better than the one trained on a traditionally designed dataset.


---
# Ab initio predictions of adsorption in flexible metal-organic frameworks for water harvesting applications.

## 用于集水应用的柔性金属有机框架中吸附的从头算预测。

Link: https://dx.doi.org/10.26434/chemrxiv-2024-lpm8c?rft_dat=source%3Ddrss

Recently, metal-organic frameworks like MOF-303 and MOF-LA2-1 have demonstrated exceptional performance for water harvesting applications.  To enable a reticular design of such materials, an accurate prediction of the adsorption properties with chemical accuracy and fully accounting for the flexibility is crucial.  The computational prediction of water adsorption properties of metal-organic frameworks (MOFs) has become standard practice.  However their predictive power to design new materials is hindered by the limited accuracy of the used interatomic potential and the limitations on how to account for the framework flexibility. In this work, we showcase a methodology to obtain chemically accurate adsorption isotherms that fully account for the framework flexibility.  The method is founded on very accurate and efficiently trained machine learning potentials and transition matrix Monte Carlo simulations to account for framework flexibility.  By first benchmarking the reference electronic level of theory used for the training, quantitatively accurate adsorption isotherms are obtained for MOF-303, a highly topical MOF being investigated for its potential use in water harvesting applications. We show that both an accurate level of theory and a proper inclusion of local and global framework flexibility is vital in the prediction of the adsorption properties of MOF-303. The broader applicability of our methodology is demonstrated through the study of related linker-exchanged materials, MOF-333 and MOF-LA2-1.  Analyses of the density profiles of water adsorbed in these MOFs yields deeper insight into the origins and differences of the observed isotherms.  An optimal water harvester should have initial seeding sites with intermediate adsorption strength, to prevent detrimental low-pressure water uptake.  To increase the working capacity, linker extension strategies can be used while maintaining the initial seeding sites, as was done in the MOF-LA2-1.  The proposed methodology is applicable to other guest molecules and MOFs, paving the way to future rational design of MOFs with specific adsorption properties for the application at hand.


---
# PiNN: equivariant neural network suite for modelling electrochemical systems

## PiNN: 用于电化学系统建模的等变神经网络套件

Link: https://dx.doi.org/10.26434/chemrxiv-2024-zfvrz?rft_dat=source%3Ddrss

Electrochemical energy storage and conversion play an increasingly important role in electrification and sustainable development across the globe. A key challenge therein is to understand, control, and design electrochemical energy materials at atomistic precision. This requires inputs from molecular modelling powered by machine learning (ML) techniques. In this work, we have upgraded our pairwise interaction neural network Python package PiNN via introducing equivariant features to the PiNet2 architecture for fitting potential energy surfaces along with PiNet2-dipole for dipole and charge predictions as well as PiNet2-chi for generating atom-condensed charge response kernels. By benchmarking publicly accessible datasets of small molecules, crystalline materials, and liquid electrolytes, we found that the equivariant PiNet2 shows significant improvements over the original PiNet architecture and provides a state-of-the-art overall performance. Furthermore, leveraging on plug-ins such as PiNNAcLe for an adaptive learn-on-the-fly workflow in generating ML potentials and PiNNwall for modelling heterogeneous electrodes under external bias, we expect PiNN to serve as a versatile and high-performing ML-accelerated platform for molecular modelling of electrochemical systems.


---
# Alleviating Head-mounted Weight Burden for Neural Imaging in Freely-behaving Rodents Using a Helium-filled Balloon

## 使用氦气填充的气球减轻行为自由的啮齿动物神经成像的头戴重量负担

Link: https://www.researchsquare.com/article/rs-5247340/latest

The recently developed miniaturized head-mounted two-photon (2P) imaging devices have served as a valuable tool for neuroscientists, enabling real-time functional neural imaging in freely-behaving animals. Although the current 2P fiberscopes and miniscopes are lightweight, the weight of any potential additional accessories inevitably imposes a burden on the animal. Here, we present a buoyancy levitation method to alleviate head-mounted weight burden on mice. By utilizing the buoyance of a helium-filled balloon to counteract the additional weight of up to 7 g, both the motion behavior and neural activities remain unaffected by the added load. This easy-to-implement method provides a platform for studying neural network function in animals, effectively freeing them from the burden of head-mounted weight.


---
# Exploring Medical Students' Preferences and Challenges in Clinical Pharmacology Education: Insights and Improvement Strategies

## 探索医学生在临床药理学教育中的偏好和挑战: 见解和改进策略

Link: https://www.researchsquare.com/article/rs-5082426/latest

Background Medical students receive foundational knowledge in clinical pharmacology, bridging the gap between pharmacology and clinical practice. While several studies have investigated clinical pharmacology teaching methodologies, few describe the teaching and learning of clinical pharmacology in Saudi Arabia. This study aimed to explore medical students' preferences for teaching and learning methods in clinical pharmacology, identify current strengths and challenges, and provide suggestions for improvement.Methods In April 2024, a cross-sectional online survey was distributed via WhatsApp to second and third-year medical students at the University of Jeddah, KSA. The survey invitation explained the purpose, voluntary participation, and anonymity of responses, with informed consent obtained. A 24-item English questionnaire, including a Likert scale and open-ended questions, was developed based on prior research and pilot-tested by five medical students. Data were analyzed using Minitab 17, employing descriptive statistics and Chi-square (&chi;2) tests to explore variable relationships.Results Ninety out of 395 medical students completed the questionnaire (22.8% response rate). Students (43.3%, n&amp;thinsp;=&amp;thinsp;39) found the curriculum comprehensive and relevant for medication management but lacked cohesiveness. Significant challenges included understanding course content (56.7%, n&amp;thinsp;=&amp;thinsp;51) and passing exams (43.3%, n&amp;thinsp;=&amp;thinsp;39). Additionally, 56.7% (n&amp;thinsp;=&amp;thinsp;51) felt overwhelmed by the volume of information and struggled to practically apply pharmacological knowledge. Interactive teaching methods, such as case-based discussions and simulations, were deemed inadequate, and hands-on experience opportunities needed to be improved. While 43.3% (n&amp;thinsp;=&amp;thinsp;39) expressed satisfaction with their education, 36.7% (n&amp;thinsp;=&amp;thinsp;33) remained neutral, indicating uncertainty about the instruction's quality and effectiveness. Half of the respondents (50%, n&amp;thinsp;=&amp;thinsp;42) called for adding root cause analysis and systems-based approaches to improve medication safety in the curriculum. Key teaching demands: autonomic pharmacology (44.3%), pharmacokinetics/dynamics (39.8%), and cardiovascular pharmacology (37.5%).Conclusions Findings suggest the need for more cohesive clinical pharmacology curricula, enhanced interactive teaching methods, and increased use of technology and practical applications to improve learning outcomes.


---
# An Interpretable Model for Health-care Insurance Fraud Detection

## 医疗保险欺诈检测的可解释模型

Link: https://www.researchsquare.com/article/rs-5012877/latest

Healthcare insurance fraud imposes a significant financial burden on healthcare systems worldwide, with annual losses reaching billions of dollars. This study aims to improve fraud detection accuracy using machine learning techniques. Our approach consists of three key stages: data preprocessing, model training and integration, and result analysis with feature interpretation. Initially, we examined the dataset's characteristics and employed embedded and permutation methods to test the performance and runtime of single models under different feature sets, selecting the minimal number of features that could still achieve high performance. We then applied ensemble techniques, including Voting, Weighted, and Stacking methods, to combine different models and compare their performances. Feature interpretation was achieved through Partial Dependence Plots (PDP), SHAP, and LIME, allowing us to understand each feature&rsquo;s impact on the predictions. Finally, we benchmarked our approach against existing studies to evaluate its advantages and limitations. The findings demonstrate improved fraud detection accuracy and offer insights into the interpretability of machine learning models in this context.


---
# Protein sequence classification using natural language processing techniques

## 使用自然语言处理技术进行蛋白质序列分类

Link: https://www.researchsquare.com/article/rs-5045037/latest

Purpose This study aimed to improve protein sequence classification through natural language processing (NLP) techniques, addressing the need for precise, automated methods. The research focused on comparing various machine learning and deep learning models to determine the most effective approach for classifying protein sequences into 75 target classes.Methods The study evaluated models such as K-Nearest Neighbors (KNN), Multinomial Na&amp;iuml;ve Bayes, Logistic Regression, Multi-Layer Perceptron (MLP), Decision Tree, Random Forest, XGBoost, Voting and Stacking classifiers, Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and transformer models (BertForSequenceClassification, DistilBERT, and ProtBert). Performance was tested using different amino acid ranges and sequence lengths.Results The Voting classifier outperformed other models with 74% accuracy and 65% F1 score, while ProtBERT achieved 76% accuracy and 61% F1 score among transformers.Conclusion Advanced NLP techniques, particularly ensemble methods like Voting classifiers, and transformer models show significant potential in protein classification, with sufficient training data and sequence similarity management being crucial for optimal performance.


---
# TriSpectraKAN: A novel approach for COPD Detection via Lung Sound Analysis

## TriSpectraKAN: 一种通过肺音分析检测COPD的新方法

Link: https://www.researchsquare.com/article/rs-5133687/latest

This study aims to create an automated, accessible, andcost-effective diagnostic tool for Chronic Obstructive Pulmonary Disease(COPD). Traditional diagnostic methods are expensive, time-consuming,and require specialized equipment. The proposed TriSpectraKAN modelleverages audio-based lung sound features to improve early diagnosis.TriSpectraKAN is a hybrid model combining spectral features and theKolmogorov-Arnold Network (KAN) to analyze lung sounds using Mel-frequency cepstral coefficients (MFCCs), chromagram, and Mel spectro-grams. Each sub-model focuses on a different audio feature, capturingunique sonic signatures. These features are merged through a hybridnetwork for comprehensive analysis. The model, trained on a COPDdataset, was deployed on a Raspberry Pi for real-time use. TriSpec-traKAN achieved 93% accuracy, an F1 score of 0.98, precision of 0.97,and recall of 0.98. This multimodal approach captured a broad rangeof lung sound features, improving diagnosis accuracy compared to tra-ditional methods. The integration of multiple audio features in TriSpec-traKAN enhances COPD diagnosis, demonstrating the potential of AIand machine learning to transform respiratory disease diagnosis throughaccessible tools.

